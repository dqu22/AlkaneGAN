{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0]]\n",
      "CC1=CC=CC(C)=C1C\n",
      "1 & 1 & 0 & 0 & 1 & 1 & 0 & 1 & 1 & 0 & 1 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\\\\n",
      "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\\\\n",
      "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\\\\n",
      "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\\\\n",
      "0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\\\\n",
      "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\\\\n",
      "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\\\\n",
      "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\\\\n",
      "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\\\\n",
      "0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\\\\n",
      "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & \\\\\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Daniel Qu, Teresa Tamayo-Mendoza\n",
    "AM 221\n",
    "Code inspired from:\n",
    "\n",
    "DCGAN on MNIST using Keras\n",
    "Author: Rowel Atienza\n",
    "Project: https://github.com/roatienza/Deep-Learning-Experiments\n",
    "Dependencies: tensorflow 1.0 and keras 2.0\n",
    "Usage: python3 dcgan_mnist.py\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NUMCHARS = 11\n",
    "MAXLEN = 23\n",
    "\n",
    "char_to_int = {\"C\": 0, \")\": 1, \"(\": 2, \"#\": 3, \"1\": 4, \"3\": 5, \"2\": 6, \"5\": 7, \"4\": 8, \"=\": 9, \"_\": 10}\n",
    "int_to_char = {}\n",
    "\n",
    "for (key, value) in char_to_int.items():\n",
    "    int_to_char[value] = key\n",
    "\n",
    "def convert_to_string(mat):\n",
    "    ret = \"\"\n",
    "    for column in mat.T:\n",
    "        if max(column) == 0:\n",
    "            continue\n",
    "        biggest = np.argmax(column)\n",
    "        if biggest == NUMCHARS - 1:\n",
    "            continue\n",
    "        ret += int_to_char[biggest]\n",
    "    return ret\n",
    "\n",
    "def smooth(mat, epsilon):\n",
    "    ret = []\n",
    "    for column in mat.T:\n",
    "        n = len(column)\n",
    "        if sum(column) == 0:\n",
    "            ret.append(np.zeros(n))\n",
    "            continue\n",
    "        new = []\n",
    "        for entry in column:\n",
    "            if entry == 1:\n",
    "                new.append(entry - epsilon)\n",
    "            else:\n",
    "                new.append(epsilon / (n - 1))\n",
    "        ret.append(new)\n",
    "    return np.array(ret).T\n",
    "\n",
    "data = np.load(\"alkanes.npy\")\n",
    "X = []\n",
    "for line in data:\n",
    "    X.append(np.reshape(line, (MAXLEN, NUMCHARS)).T)\n",
    "    \n",
    "epsilon = 0.1\n",
    "    \n",
    "for i in range(1):\n",
    "    sample = X[i]\n",
    "    print(sample)\n",
    "#     print(smooth(sample, epsilon))\n",
    "    print(convert_to_string(sample))\n",
    "    \n",
    "\n",
    "# add noise to discriminator input, decay noise with time?\n",
    "\n",
    "def tex(mat):\n",
    "    ret = \"\"\n",
    "    for line in mat:\n",
    "        for entry in line:\n",
    "            ret += str(entry) + \" & \"\n",
    "        ret += \"\\\\\\\\\"\n",
    "        ret += \"\\n\"\n",
    "    return ret\n",
    "\n",
    "print(tex(X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_24 (Conv2D)           (None, 6, 12, 64)         1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 6, 12, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 6, 12, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 3, 6, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 3, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 3, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 2305      \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 208,897\n",
      "Trainable params: 208,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 4608)              465408    \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 4608)              18432     \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "reshape_11 (Reshape)         (None, 3, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 3, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_21 (UpSampling (None, 6, 12, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_32 (Conv2DT (None, 6, 12, 128)        819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 6, 12, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 6, 12, 128)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_22 (UpSampling (None, 12, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_33 (Conv2DT (None, 12, 24, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 12, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 12, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_34 (Conv2DT (None, 12, 24, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 12, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 12, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_35 (Conv2DT (None, 12, 24, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 12, 24, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,560,961\n",
      "Trainable params: 1,551,297\n",
      "Non-trainable params: 9,664\n",
      "_________________________________________________________________\n",
      "0: [D loss: 0.698439, acc: 0.363281]  [A loss: 1.262244, acc: 0.000000]\n",
      "1: [D loss: 0.657024, acc: 0.500000]  [A loss: 0.662838, acc: 0.945312]\n",
      "2: [D loss: 0.759102, acc: 0.500000]  [A loss: 0.676123, acc: 0.781250]\n",
      "3: [D loss: 0.688823, acc: 0.500000]  [A loss: 0.700360, acc: 0.421875]\n",
      "4: [D loss: 0.724029, acc: 0.500000]  [A loss: 0.818218, acc: 0.000000]\n",
      "5: [D loss: 0.625392, acc: 0.500000]  [A loss: 0.754528, acc: 0.007812]\n",
      "6: [D loss: 0.607392, acc: 0.503906]  [A loss: 0.768173, acc: 0.031250]\n",
      "7: [D loss: 0.637990, acc: 0.500000]  [A loss: 0.745005, acc: 0.054688]\n",
      "8: [D loss: 0.578927, acc: 0.503906]  [A loss: 0.752073, acc: 0.031250]\n",
      "9: [D loss: 0.601777, acc: 0.500000]  [A loss: 0.701908, acc: 0.390625]\n",
      "10: [D loss: 0.602564, acc: 0.500000]  [A loss: 0.760768, acc: 0.046875]\n",
      "11: [D loss: 0.601360, acc: 0.500000]  [A loss: 0.707580, acc: 0.406250]\n",
      "12: [D loss: 0.589516, acc: 0.500000]  [A loss: 0.708837, acc: 0.359375]\n",
      "13: [D loss: 0.573859, acc: 0.511719]  [A loss: 0.731201, acc: 0.242188]\n",
      "14: [D loss: 0.560662, acc: 0.523438]  [A loss: 0.740381, acc: 0.078125]\n",
      "15: [D loss: 0.562703, acc: 0.531250]  [A loss: 0.777518, acc: 0.007812]\n",
      "16: [D loss: 0.538369, acc: 0.757812]  [A loss: 0.753782, acc: 0.039062]\n",
      "17: [D loss: 0.574029, acc: 0.511719]  [A loss: 0.811971, acc: 0.000000]\n",
      "18: [D loss: 0.496804, acc: 1.000000]  [A loss: 0.651368, acc: 0.906250]\n",
      "19: [D loss: 0.597947, acc: 0.500000]  [A loss: 0.877011, acc: 0.000000]\n",
      "20: [D loss: 0.527847, acc: 0.984375]  [A loss: 0.706771, acc: 0.375000]\n",
      "21: [D loss: 0.536027, acc: 0.562500]  [A loss: 0.805406, acc: 0.000000]\n",
      "22: [D loss: 0.532529, acc: 0.906250]  [A loss: 0.754759, acc: 0.015625]\n",
      "23: [D loss: 0.499127, acc: 0.937500]  [A loss: 0.790595, acc: 0.000000]\n",
      "24: [D loss: 0.528096, acc: 0.640625]  [A loss: 0.874508, acc: 0.000000]\n",
      "25: [D loss: 0.480748, acc: 0.996094]  [A loss: 0.802524, acc: 0.007812]\n",
      "26: [D loss: 0.503847, acc: 0.617188]  [A loss: 0.985829, acc: 0.000000]\n",
      "27: [D loss: 0.447593, acc: 1.000000]  [A loss: 0.779388, acc: 0.015625]\n",
      "28: [D loss: 0.482788, acc: 0.554688]  [A loss: 1.081233, acc: 0.000000]\n",
      "29: [D loss: 0.429659, acc: 0.996094]  [A loss: 0.805624, acc: 0.000000]\n",
      "30: [D loss: 0.458881, acc: 0.554688]  [A loss: 1.124636, acc: 0.000000]\n",
      "31: [D loss: 0.407067, acc: 0.984375]  [A loss: 0.882589, acc: 0.000000]\n",
      "32: [D loss: 0.422000, acc: 0.796875]  [A loss: 1.170621, acc: 0.000000]\n",
      "33: [D loss: 0.379241, acc: 0.992188]  [A loss: 0.953107, acc: 0.000000]\n",
      "34: [D loss: 0.401729, acc: 0.886719]  [A loss: 1.236010, acc: 0.000000]\n",
      "35: [D loss: 0.375309, acc: 0.992188]  [A loss: 1.060741, acc: 0.000000]\n",
      "36: [D loss: 0.537782, acc: 0.500000]  [A loss: 1.346230, acc: 0.000000]\n",
      "37: [D loss: 0.500925, acc: 0.816406]  [A loss: 1.184819, acc: 0.000000]\n",
      "38: [D loss: 0.416538, acc: 0.964844]  [A loss: 1.349437, acc: 0.000000]\n",
      "39: [D loss: 0.321687, acc: 1.000000]  [A loss: 1.247211, acc: 0.000000]\n",
      "40: [D loss: 0.390073, acc: 0.910156]  [A loss: 1.730110, acc: 0.000000]\n",
      "41: [D loss: 0.314802, acc: 0.988281]  [A loss: 1.373635, acc: 0.000000]\n",
      "42: [D loss: 0.397551, acc: 0.875000]  [A loss: 2.213819, acc: 0.000000]\n",
      "43: [D loss: 0.275396, acc: 0.980469]  [A loss: 1.356905, acc: 0.000000]\n",
      "44: [D loss: 0.324748, acc: 0.996094]  [A loss: 1.791967, acc: 0.000000]\n",
      "45: [D loss: 0.260593, acc: 0.992188]  [A loss: 1.486298, acc: 0.000000]\n",
      "46: [D loss: 0.823608, acc: 0.496094]  [A loss: 1.665486, acc: 0.000000]\n",
      "47: [D loss: 0.856928, acc: 0.386719]  [A loss: 1.436669, acc: 0.000000]\n",
      "48: [D loss: 0.680239, acc: 0.457031]  [A loss: 1.108399, acc: 0.000000]\n",
      "49: [D loss: 1.199953, acc: 0.406250]  [A loss: 1.740431, acc: 0.000000]\n",
      "50: [D loss: 0.865090, acc: 0.281250]  [A loss: 1.778202, acc: 0.000000]\n",
      "51: [D loss: 0.681753, acc: 0.496094]  [A loss: 1.491010, acc: 0.000000]\n",
      "52: [D loss: 0.591755, acc: 0.578125]  [A loss: 1.993633, acc: 0.000000]\n",
      "53: [D loss: 0.431490, acc: 0.929688]  [A loss: 1.559170, acc: 0.000000]\n",
      "54: [D loss: 0.441191, acc: 0.808594]  [A loss: 2.007508, acc: 0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55: [D loss: 0.333848, acc: 0.972656]  [A loss: 1.643876, acc: 0.000000]\n",
      "56: [D loss: 0.352974, acc: 0.972656]  [A loss: 1.694229, acc: 0.000000]\n",
      "57: [D loss: 0.310990, acc: 0.988281]  [A loss: 1.796386, acc: 0.000000]\n",
      "58: [D loss: 0.258764, acc: 1.000000]  [A loss: 1.525250, acc: 0.000000]\n",
      "59: [D loss: 0.380398, acc: 0.839844]  [A loss: 2.330662, acc: 0.000000]\n",
      "60: [D loss: 0.328140, acc: 0.996094]  [A loss: 1.041507, acc: 0.007812]\n",
      "61: [D loss: 0.637032, acc: 0.500000]  [A loss: 1.683642, acc: 0.000000]\n",
      "62: [D loss: 0.494520, acc: 0.816406]  [A loss: 0.749008, acc: 0.367188]\n",
      "63: [D loss: 0.833677, acc: 0.433594]  [A loss: 1.771343, acc: 0.000000]\n",
      "64: [D loss: 0.632089, acc: 0.718750]  [A loss: 1.098801, acc: 0.000000]\n",
      "65: [D loss: 0.667169, acc: 0.484375]  [A loss: 1.734233, acc: 0.000000]\n",
      "66: [D loss: 0.575152, acc: 0.820312]  [A loss: 1.105397, acc: 0.000000]\n",
      "67: [D loss: 0.613848, acc: 0.515625]  [A loss: 1.915534, acc: 0.000000]\n",
      "68: [D loss: 0.469616, acc: 0.863281]  [A loss: 1.087232, acc: 0.007812]\n",
      "69: [D loss: 0.569169, acc: 0.539062]  [A loss: 2.113536, acc: 0.000000]\n",
      "70: [D loss: 0.411214, acc: 0.886719]  [A loss: 1.099973, acc: 0.015625]\n",
      "71: [D loss: 0.860528, acc: 0.484375]  [A loss: 2.600122, acc: 0.000000]\n",
      "72: [D loss: 0.534684, acc: 0.773438]  [A loss: 1.022751, acc: 0.015625]\n",
      "73: [D loss: 0.603598, acc: 0.511719]  [A loss: 1.550719, acc: 0.000000]\n",
      "74: [D loss: 0.484762, acc: 0.890625]  [A loss: 1.244989, acc: 0.007812]\n",
      "75: [D loss: 0.527206, acc: 0.703125]  [A loss: 1.611806, acc: 0.000000]\n",
      "76: [D loss: 0.403785, acc: 0.933594]  [A loss: 1.057096, acc: 0.039062]\n",
      "77: [D loss: 0.565325, acc: 0.613281]  [A loss: 1.803294, acc: 0.000000]\n",
      "78: [D loss: 0.350475, acc: 0.933594]  [A loss: 0.862945, acc: 0.265625]\n",
      "79: [D loss: 0.617964, acc: 0.527344]  [A loss: 1.914736, acc: 0.000000]\n",
      "80: [D loss: 0.414754, acc: 0.898438]  [A loss: 0.712814, acc: 0.492188]\n",
      "81: [D loss: 2.411469, acc: 0.464844]  [A loss: 0.891534, acc: 0.265625]\n",
      "82: [D loss: 2.155089, acc: 0.148438]  [A loss: 1.191262, acc: 0.085938]\n",
      "83: [D loss: 2.110489, acc: 0.027344]  [A loss: 1.583291, acc: 0.000000]\n",
      "84: [D loss: 1.486724, acc: 0.003906]  [A loss: 1.102531, acc: 0.078125]\n",
      "85: [D loss: 1.176042, acc: 0.050781]  [A loss: 1.082797, acc: 0.148438]\n",
      "86: [D loss: 1.062666, acc: 0.121094]  [A loss: 1.349591, acc: 0.007812]\n",
      "87: [D loss: 0.854117, acc: 0.300781]  [A loss: 1.341777, acc: 0.039062]\n",
      "88: [D loss: 0.739407, acc: 0.488281]  [A loss: 1.530109, acc: 0.000000]\n",
      "89: [D loss: 0.745948, acc: 0.496094]  [A loss: 2.069803, acc: 0.000000]\n",
      "90: [D loss: 0.512173, acc: 0.816406]  [A loss: 0.701687, acc: 0.531250]\n",
      "91: [D loss: 0.999410, acc: 0.476562]  [A loss: 3.606897, acc: 0.000000]\n",
      "92: [D loss: 0.606049, acc: 0.714844]  [A loss: 1.127142, acc: 0.093750]\n",
      "93: [D loss: 0.833549, acc: 0.472656]  [A loss: 1.717901, acc: 0.000000]\n",
      "94: [D loss: 0.630630, acc: 0.699219]  [A loss: 0.685381, acc: 0.593750]\n",
      "95: [D loss: 0.915471, acc: 0.441406]  [A loss: 1.899045, acc: 0.000000]\n",
      "96: [D loss: 0.647850, acc: 0.621094]  [A loss: 0.707268, acc: 0.539062]\n",
      "97: [D loss: 0.838385, acc: 0.472656]  [A loss: 1.458743, acc: 0.000000]\n",
      "98: [D loss: 0.695535, acc: 0.566406]  [A loss: 0.853862, acc: 0.195312]\n",
      "99: [D loss: 0.775542, acc: 0.394531]  [A loss: 1.296098, acc: 0.031250]\n",
      "100: [D loss: 0.738777, acc: 0.476562]  [A loss: 1.080047, acc: 0.078125]\n",
      "101: [D loss: 0.745328, acc: 0.445312]  [A loss: 1.178640, acc: 0.023438]\n",
      "102: [D loss: 0.687435, acc: 0.605469]  [A loss: 1.081010, acc: 0.046875]\n",
      "103: [D loss: 0.648431, acc: 0.640625]  [A loss: 1.121694, acc: 0.007812]\n",
      "104: [D loss: 0.653213, acc: 0.605469]  [A loss: 1.372580, acc: 0.000000]\n",
      "105: [D loss: 0.594758, acc: 0.769531]  [A loss: 0.977300, acc: 0.054688]\n",
      "106: [D loss: 0.676647, acc: 0.562500]  [A loss: 1.417446, acc: 0.007812]\n",
      "107: [D loss: 0.550584, acc: 0.796875]  [A loss: 0.784797, acc: 0.359375]\n",
      "108: [D loss: 0.720454, acc: 0.496094]  [A loss: 1.618068, acc: 0.000000]\n",
      "109: [D loss: 0.571988, acc: 0.738281]  [A loss: 0.829250, acc: 0.281250]\n",
      "110: [D loss: 0.651966, acc: 0.539062]  [A loss: 1.472381, acc: 0.000000]\n",
      "111: [D loss: 0.564352, acc: 0.777344]  [A loss: 0.944002, acc: 0.117188]\n",
      "112: [D loss: 0.588813, acc: 0.617188]  [A loss: 1.379746, acc: 0.007812]\n",
      "113: [D loss: 0.509807, acc: 0.863281]  [A loss: 1.077815, acc: 0.031250]\n",
      "114: [D loss: 0.532354, acc: 0.734375]  [A loss: 1.332242, acc: 0.000000]\n",
      "115: [D loss: 0.511339, acc: 0.816406]  [A loss: 1.073987, acc: 0.046875]\n",
      "116: [D loss: 0.519902, acc: 0.753906]  [A loss: 1.355234, acc: 0.000000]\n",
      "117: [D loss: 0.452670, acc: 0.890625]  [A loss: 1.052265, acc: 0.085938]\n",
      "118: [D loss: 0.532235, acc: 0.707031]  [A loss: 1.781265, acc: 0.000000]\n",
      "119: [D loss: 0.486648, acc: 0.824219]  [A loss: 0.981167, acc: 0.164062]\n",
      "120: [D loss: 0.896376, acc: 0.496094]  [A loss: 1.781757, acc: 0.000000]\n",
      "121: [D loss: 0.654002, acc: 0.664062]  [A loss: 0.909696, acc: 0.132812]\n",
      "122: [D loss: 0.735964, acc: 0.503906]  [A loss: 1.477391, acc: 0.000000]\n",
      "123: [D loss: 0.628222, acc: 0.660156]  [A loss: 1.274176, acc: 0.031250]\n",
      "124: [D loss: 0.720063, acc: 0.492188]  [A loss: 1.651322, acc: 0.000000]\n",
      "125: [D loss: 0.620765, acc: 0.656250]  [A loss: 1.431980, acc: 0.000000]\n",
      "126: [D loss: 0.665759, acc: 0.550781]  [A loss: 1.628647, acc: 0.000000]\n",
      "127: [D loss: 0.484542, acc: 0.851562]  [A loss: 0.938387, acc: 0.148438]\n",
      "128: [D loss: 0.700455, acc: 0.519531]  [A loss: 2.143276, acc: 0.000000]\n",
      "129: [D loss: 0.534499, acc: 0.808594]  [A loss: 0.937229, acc: 0.125000]\n",
      "130: [D loss: 0.678606, acc: 0.507812]  [A loss: 1.540925, acc: 0.000000]\n",
      "131: [D loss: 0.550757, acc: 0.832031]  [A loss: 1.046263, acc: 0.070312]\n",
      "132: [D loss: 0.550515, acc: 0.660156]  [A loss: 1.490068, acc: 0.000000]\n",
      "133: [D loss: 0.477603, acc: 0.863281]  [A loss: 1.213762, acc: 0.031250]\n",
      "134: [D loss: 0.460933, acc: 0.828125]  [A loss: 1.449360, acc: 0.000000]\n",
      "135: [D loss: 0.454836, acc: 0.863281]  [A loss: 1.275354, acc: 0.023438]\n",
      "136: [D loss: 0.448925, acc: 0.828125]  [A loss: 1.578483, acc: 0.000000]\n",
      "137: [D loss: 0.393422, acc: 0.945312]  [A loss: 1.203262, acc: 0.023438]\n",
      "138: [D loss: 0.447255, acc: 0.796875]  [A loss: 1.779710, acc: 0.000000]\n",
      "139: [D loss: 0.379644, acc: 0.945312]  [A loss: 1.170404, acc: 0.046875]\n",
      "140: [D loss: 0.476867, acc: 0.703125]  [A loss: 1.937233, acc: 0.000000]\n",
      "141: [D loss: 0.416443, acc: 0.902344]  [A loss: 0.995802, acc: 0.171875]\n",
      "142: [D loss: 0.491448, acc: 0.734375]  [A loss: 1.743520, acc: 0.000000]\n",
      "143: [D loss: 0.361903, acc: 0.929688]  [A loss: 1.087976, acc: 0.140625]\n",
      "144: [D loss: 0.488184, acc: 0.691406]  [A loss: 1.769452, acc: 0.015625]\n",
      "145: [D loss: 0.421425, acc: 0.863281]  [A loss: 1.018186, acc: 0.226562]\n",
      "146: [D loss: 0.477858, acc: 0.750000]  [A loss: 1.775492, acc: 0.007812]\n",
      "147: [D loss: 0.437953, acc: 0.859375]  [A loss: 1.176633, acc: 0.109375]\n",
      "148: [D loss: 0.566108, acc: 0.644531]  [A loss: 1.715026, acc: 0.007812]\n",
      "149: [D loss: 0.515778, acc: 0.789062]  [A loss: 0.831304, acc: 0.382812]\n",
      "150: [D loss: 0.640421, acc: 0.582031]  [A loss: 1.713729, acc: 0.007812]\n",
      "151: [D loss: 0.615428, acc: 0.656250]  [A loss: 0.806793, acc: 0.429688]\n",
      "152: [D loss: 0.854746, acc: 0.527344]  [A loss: 2.112743, acc: 0.007812]\n",
      "153: [D loss: 0.599159, acc: 0.667969]  [A loss: 0.830914, acc: 0.492188]\n",
      "154: [D loss: 0.800291, acc: 0.546875]  [A loss: 1.604464, acc: 0.046875]\n",
      "155: [D loss: 0.677493, acc: 0.613281]  [A loss: 1.114118, acc: 0.195312]\n",
      "156: [D loss: 0.663360, acc: 0.585938]  [A loss: 1.530114, acc: 0.070312]\n",
      "157: [D loss: 0.587214, acc: 0.714844]  [A loss: 1.223803, acc: 0.203125]\n",
      "158: [D loss: 0.548789, acc: 0.777344]  [A loss: 1.319307, acc: 0.109375]\n",
      "159: [D loss: 0.574653, acc: 0.742188]  [A loss: 1.308729, acc: 0.078125]\n",
      "160: [D loss: 0.505823, acc: 0.761719]  [A loss: 1.163528, acc: 0.164062]\n",
      "161: [D loss: 0.539501, acc: 0.761719]  [A loss: 1.419854, acc: 0.023438]\n",
      "162: [D loss: 0.494199, acc: 0.785156]  [A loss: 1.192531, acc: 0.101562]\n",
      "163: [D loss: 0.501760, acc: 0.742188]  [A loss: 1.501693, acc: 0.023438]\n",
      "164: [D loss: 0.419137, acc: 0.890625]  [A loss: 1.258071, acc: 0.062500]\n",
      "165: [D loss: 0.489029, acc: 0.757812]  [A loss: 1.638734, acc: 0.007812]\n",
      "166: [D loss: 0.428908, acc: 0.847656]  [A loss: 1.178087, acc: 0.101562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167: [D loss: 0.537854, acc: 0.671875]  [A loss: 1.936297, acc: 0.000000]\n",
      "168: [D loss: 0.413226, acc: 0.894531]  [A loss: 1.144629, acc: 0.156250]\n",
      "169: [D loss: 0.522942, acc: 0.660156]  [A loss: 2.053113, acc: 0.000000]\n",
      "170: [D loss: 0.413120, acc: 0.882812]  [A loss: 1.132554, acc: 0.164062]\n",
      "171: [D loss: 0.481911, acc: 0.714844]  [A loss: 2.141959, acc: 0.000000]\n",
      "172: [D loss: 0.361832, acc: 0.906250]  [A loss: 1.427312, acc: 0.023438]\n",
      "173: [D loss: 0.462509, acc: 0.773438]  [A loss: 2.052840, acc: 0.000000]\n",
      "174: [D loss: 0.406225, acc: 0.867188]  [A loss: 1.490224, acc: 0.023438]\n",
      "175: [D loss: 0.933333, acc: 0.589844]  [A loss: 2.449187, acc: 0.000000]\n",
      "176: [D loss: 1.422307, acc: 0.191406]  [A loss: 2.368757, acc: 0.000000]\n",
      "177: [D loss: 1.144376, acc: 0.226562]  [A loss: 1.367906, acc: 0.054688]\n",
      "178: [D loss: 1.168979, acc: 0.203125]  [A loss: 2.197627, acc: 0.000000]\n",
      "179: [D loss: 0.882998, acc: 0.410156]  [A loss: 1.209328, acc: 0.117188]\n",
      "180: [D loss: 1.131648, acc: 0.351562]  [A loss: 2.599833, acc: 0.000000]\n",
      "181: [D loss: 0.667779, acc: 0.640625]  [A loss: 1.050963, acc: 0.210938]\n",
      "182: [D loss: 0.896697, acc: 0.476562]  [A loss: 2.286878, acc: 0.000000]\n",
      "183: [D loss: 0.585559, acc: 0.703125]  [A loss: 1.038492, acc: 0.265625]\n",
      "184: [D loss: 0.703079, acc: 0.574219]  [A loss: 1.938548, acc: 0.007812]\n",
      "185: [D loss: 0.510198, acc: 0.765625]  [A loss: 1.106579, acc: 0.187500]\n",
      "186: [D loss: 0.641222, acc: 0.660156]  [A loss: 1.590588, acc: 0.023438]\n",
      "187: [D loss: 0.523763, acc: 0.773438]  [A loss: 1.224088, acc: 0.156250]\n",
      "188: [D loss: 0.632958, acc: 0.660156]  [A loss: 1.631965, acc: 0.054688]\n",
      "189: [D loss: 0.432353, acc: 0.886719]  [A loss: 0.845954, acc: 0.375000]\n",
      "190: [D loss: 0.799415, acc: 0.550781]  [A loss: 1.909827, acc: 0.007812]\n",
      "191: [D loss: 0.527176, acc: 0.800781]  [A loss: 0.900956, acc: 0.296875]\n",
      "192: [D loss: 0.693490, acc: 0.554688]  [A loss: 1.603229, acc: 0.007812]\n",
      "193: [D loss: 0.526096, acc: 0.777344]  [A loss: 1.090587, acc: 0.093750]\n",
      "194: [D loss: 0.604794, acc: 0.644531]  [A loss: 1.555891, acc: 0.015625]\n",
      "195: [D loss: 0.558791, acc: 0.714844]  [A loss: 1.400340, acc: 0.039062]\n",
      "196: [D loss: 0.586521, acc: 0.671875]  [A loss: 1.694810, acc: 0.007812]\n",
      "197: [D loss: 0.709740, acc: 0.554688]  [A loss: 1.616190, acc: 0.023438]\n",
      "198: [D loss: 1.270558, acc: 0.316406]  [A loss: 2.273391, acc: 0.000000]\n",
      "199: [D loss: 0.929737, acc: 0.351562]  [A loss: 1.255866, acc: 0.078125]\n",
      "200: [D loss: 1.005763, acc: 0.367188]  [A loss: 2.675967, acc: 0.000000]\n",
      "201: [D loss: 0.731551, acc: 0.578125]  [A loss: 0.869810, acc: 0.296875]\n",
      "202: [D loss: 0.934190, acc: 0.453125]  [A loss: 2.393906, acc: 0.000000]\n",
      "203: [D loss: 0.591688, acc: 0.722656]  [A loss: 0.894687, acc: 0.234375]\n",
      "204: [D loss: 0.884113, acc: 0.460938]  [A loss: 1.941412, acc: 0.000000]\n",
      "205: [D loss: 0.632086, acc: 0.671875]  [A loss: 1.124619, acc: 0.117188]\n",
      "206: [D loss: 0.701961, acc: 0.535156]  [A loss: 1.599194, acc: 0.007812]\n",
      "207: [D loss: 0.613506, acc: 0.691406]  [A loss: 1.089849, acc: 0.117188]\n",
      "208: [D loss: 0.691335, acc: 0.562500]  [A loss: 1.476381, acc: 0.007812]\n",
      "209: [D loss: 0.602385, acc: 0.714844]  [A loss: 0.930586, acc: 0.242188]\n",
      "210: [D loss: 0.699307, acc: 0.531250]  [A loss: 1.486933, acc: 0.015625]\n",
      "211: [D loss: 0.597543, acc: 0.726562]  [A loss: 0.890864, acc: 0.265625]\n",
      "212: [D loss: 0.709489, acc: 0.562500]  [A loss: 1.483429, acc: 0.000000]\n",
      "213: [D loss: 0.581918, acc: 0.707031]  [A loss: 1.066094, acc: 0.117188]\n",
      "214: [D loss: 0.608007, acc: 0.617188]  [A loss: 1.251142, acc: 0.031250]\n",
      "215: [D loss: 0.586000, acc: 0.644531]  [A loss: 1.139651, acc: 0.046875]\n",
      "216: [D loss: 0.611621, acc: 0.597656]  [A loss: 1.395838, acc: 0.015625]\n",
      "217: [D loss: 0.602442, acc: 0.703125]  [A loss: 1.059943, acc: 0.101562]\n",
      "218: [D loss: 0.624633, acc: 0.632812]  [A loss: 1.193466, acc: 0.046875]\n",
      "219: [D loss: 0.489912, acc: 0.808594]  [A loss: 1.323396, acc: 0.054688]\n",
      "220: [D loss: 0.570031, acc: 0.648438]  [A loss: 1.478091, acc: 0.000000]\n",
      "221: [D loss: 0.619830, acc: 0.648438]  [A loss: 1.187573, acc: 0.046875]\n",
      "222: [D loss: 0.640929, acc: 0.656250]  [A loss: 1.135058, acc: 0.039062]\n",
      "223: [D loss: 0.574177, acc: 0.687500]  [A loss: 1.251028, acc: 0.023438]\n",
      "224: [D loss: 0.555734, acc: 0.695312]  [A loss: 1.275057, acc: 0.015625]\n",
      "225: [D loss: 0.573525, acc: 0.734375]  [A loss: 1.056210, acc: 0.078125]\n",
      "226: [D loss: 0.524870, acc: 0.664062]  [A loss: 1.606854, acc: 0.000000]\n",
      "227: [D loss: 0.514249, acc: 0.816406]  [A loss: 0.952883, acc: 0.164062]\n",
      "228: [D loss: 0.541502, acc: 0.664062]  [A loss: 1.651819, acc: 0.000000]\n",
      "229: [D loss: 0.462968, acc: 0.839844]  [A loss: 1.061059, acc: 0.101562]\n",
      "230: [D loss: 0.505684, acc: 0.691406]  [A loss: 1.772278, acc: 0.000000]\n",
      "231: [D loss: 0.419794, acc: 0.894531]  [A loss: 1.202692, acc: 0.054688]\n",
      "232: [D loss: 0.450647, acc: 0.792969]  [A loss: 1.667605, acc: 0.007812]\n",
      "233: [D loss: 0.373493, acc: 0.902344]  [A loss: 1.433335, acc: 0.000000]\n",
      "234: [D loss: 0.404156, acc: 0.855469]  [A loss: 1.686541, acc: 0.007812]\n",
      "235: [D loss: 0.367776, acc: 0.906250]  [A loss: 1.434427, acc: 0.007812]\n",
      "236: [D loss: 0.691393, acc: 0.531250]  [A loss: 2.386695, acc: 0.000000]\n",
      "237: [D loss: 0.867695, acc: 0.441406]  [A loss: 2.350968, acc: 0.000000]\n",
      "238: [D loss: 0.701345, acc: 0.621094]  [A loss: 1.451346, acc: 0.015625]\n",
      "239: [D loss: 0.718030, acc: 0.511719]  [A loss: 2.051795, acc: 0.007812]\n",
      "240: [D loss: 0.568346, acc: 0.746094]  [A loss: 1.247285, acc: 0.125000]\n",
      "241: [D loss: 0.712227, acc: 0.582031]  [A loss: 2.473551, acc: 0.000000]\n",
      "242: [D loss: 0.469009, acc: 0.808594]  [A loss: 1.088615, acc: 0.242188]\n",
      "243: [D loss: 1.567131, acc: 0.468750]  [A loss: 3.163883, acc: 0.000000]\n",
      "244: [D loss: 0.627604, acc: 0.671875]  [A loss: 1.107088, acc: 0.234375]\n",
      "245: [D loss: 1.322452, acc: 0.433594]  [A loss: 2.167178, acc: 0.015625]\n",
      "246: [D loss: 1.002266, acc: 0.390625]  [A loss: 1.726317, acc: 0.039062]\n",
      "247: [D loss: 1.059621, acc: 0.289062]  [A loss: 1.416214, acc: 0.039062]\n",
      "248: [D loss: 0.965783, acc: 0.328125]  [A loss: 1.277198, acc: 0.070312]\n",
      "249: [D loss: 0.872062, acc: 0.425781]  [A loss: 1.283252, acc: 0.093750]\n",
      "250: [D loss: 0.877770, acc: 0.441406]  [A loss: 1.586283, acc: 0.000000]\n",
      "251: [D loss: 0.678085, acc: 0.589844]  [A loss: 0.954284, acc: 0.242188]\n",
      "252: [D loss: 0.929276, acc: 0.414062]  [A loss: 2.190346, acc: 0.000000]\n",
      "253: [D loss: 0.648493, acc: 0.644531]  [A loss: 0.781257, acc: 0.460938]\n",
      "254: [D loss: 0.912560, acc: 0.437500]  [A loss: 1.701840, acc: 0.000000]\n",
      "255: [D loss: 0.639893, acc: 0.714844]  [A loss: 0.815473, acc: 0.414062]\n",
      "256: [D loss: 0.853726, acc: 0.480469]  [A loss: 1.705433, acc: 0.015625]\n",
      "257: [D loss: 0.624643, acc: 0.683594]  [A loss: 1.094613, acc: 0.210938]\n",
      "258: [D loss: 0.739046, acc: 0.515625]  [A loss: 1.685385, acc: 0.007812]\n",
      "259: [D loss: 0.647634, acc: 0.613281]  [A loss: 1.400246, acc: 0.085938]\n",
      "260: [D loss: 0.657843, acc: 0.640625]  [A loss: 1.835253, acc: 0.007812]\n",
      "261: [D loss: 0.565934, acc: 0.691406]  [A loss: 1.279822, acc: 0.125000]\n",
      "262: [D loss: 0.636675, acc: 0.636719]  [A loss: 2.143972, acc: 0.000000]\n",
      "263: [D loss: 0.485887, acc: 0.812500]  [A loss: 1.453020, acc: 0.078125]\n",
      "264: [D loss: 0.562400, acc: 0.687500]  [A loss: 1.956160, acc: 0.000000]\n",
      "265: [D loss: 0.456933, acc: 0.820312]  [A loss: 1.263022, acc: 0.101562]\n",
      "266: [D loss: 0.521101, acc: 0.710938]  [A loss: 1.713817, acc: 0.007812]\n",
      "267: [D loss: 0.403043, acc: 0.863281]  [A loss: 1.121875, acc: 0.132812]\n",
      "268: [D loss: 0.527226, acc: 0.652344]  [A loss: 1.995363, acc: 0.000000]\n",
      "269: [D loss: 0.471185, acc: 0.820312]  [A loss: 1.104045, acc: 0.093750]\n",
      "270: [D loss: 0.604796, acc: 0.589844]  [A loss: 1.598360, acc: 0.015625]\n",
      "271: [D loss: 0.505347, acc: 0.765625]  [A loss: 1.238703, acc: 0.093750]\n",
      "272: [D loss: 0.471566, acc: 0.820312]  [A loss: 1.398470, acc: 0.062500]\n",
      "273: [D loss: 0.570463, acc: 0.679688]  [A loss: 1.876023, acc: 0.000000]\n",
      "274: [D loss: 0.433350, acc: 0.863281]  [A loss: 1.193559, acc: 0.109375]\n",
      "275: [D loss: 0.555617, acc: 0.660156]  [A loss: 1.830202, acc: 0.007812]\n",
      "276: [D loss: 0.475938, acc: 0.808594]  [A loss: 1.345689, acc: 0.046875]\n",
      "277: [D loss: 0.475079, acc: 0.777344]  [A loss: 1.941496, acc: 0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278: [D loss: 0.399147, acc: 0.867188]  [A loss: 1.601343, acc: 0.000000]\n",
      "279: [D loss: 0.425364, acc: 0.828125]  [A loss: 1.948880, acc: 0.007812]\n",
      "280: [D loss: 0.371470, acc: 0.867188]  [A loss: 1.512353, acc: 0.015625]\n",
      "281: [D loss: 0.408567, acc: 0.843750]  [A loss: 2.120075, acc: 0.000000]\n",
      "282: [D loss: 0.363542, acc: 0.843750]  [A loss: 1.355899, acc: 0.054688]\n",
      "283: [D loss: 0.418344, acc: 0.777344]  [A loss: 2.471838, acc: 0.000000]\n",
      "284: [D loss: 0.281276, acc: 0.929688]  [A loss: 1.238841, acc: 0.070312]\n",
      "285: [D loss: 0.406453, acc: 0.785156]  [A loss: 2.346325, acc: 0.000000]\n",
      "286: [D loss: 0.293991, acc: 0.910156]  [A loss: 0.790503, acc: 0.406250]\n",
      "287: [D loss: 0.798918, acc: 0.519531]  [A loss: 2.680184, acc: 0.000000]\n",
      "288: [D loss: 0.388926, acc: 0.835938]  [A loss: 1.024309, acc: 0.351562]\n",
      "289: [D loss: 1.094714, acc: 0.539062]  [A loss: 2.140266, acc: 0.007812]\n",
      "290: [D loss: 0.677112, acc: 0.660156]  [A loss: 0.667853, acc: 0.578125]\n",
      "291: [D loss: 0.825244, acc: 0.484375]  [A loss: 1.420355, acc: 0.085938]\n",
      "292: [D loss: 0.845814, acc: 0.449219]  [A loss: 1.497530, acc: 0.062500]\n",
      "293: [D loss: 0.923981, acc: 0.433594]  [A loss: 1.266865, acc: 0.117188]\n",
      "294: [D loss: 0.953793, acc: 0.410156]  [A loss: 1.544288, acc: 0.070312]\n",
      "295: [D loss: 0.697444, acc: 0.644531]  [A loss: 0.664153, acc: 0.554688]\n",
      "296: [D loss: 1.297450, acc: 0.390625]  [A loss: 2.722911, acc: 0.000000]\n",
      "297: [D loss: 0.780640, acc: 0.593750]  [A loss: 0.950785, acc: 0.242188]\n",
      "298: [D loss: 0.905947, acc: 0.468750]  [A loss: 1.558504, acc: 0.015625]\n",
      "299: [D loss: 0.766913, acc: 0.515625]  [A loss: 1.072718, acc: 0.187500]\n",
      "300: [D loss: 0.795111, acc: 0.464844]  [A loss: 1.161033, acc: 0.140625]\n",
      "301: [D loss: 0.823263, acc: 0.484375]  [A loss: 0.999965, acc: 0.289062]\n",
      "302: [D loss: 0.824843, acc: 0.445312]  [A loss: 1.268978, acc: 0.085938]\n",
      "303: [D loss: 0.767021, acc: 0.511719]  [A loss: 1.004807, acc: 0.218750]\n",
      "304: [D loss: 0.847034, acc: 0.460938]  [A loss: 1.227505, acc: 0.125000]\n",
      "305: [D loss: 0.888726, acc: 0.437500]  [A loss: 0.940183, acc: 0.328125]\n",
      "306: [D loss: 0.867329, acc: 0.425781]  [A loss: 1.196443, acc: 0.101562]\n",
      "307: [D loss: 0.711514, acc: 0.542969]  [A loss: 1.044287, acc: 0.234375]\n",
      "308: [D loss: 0.781528, acc: 0.472656]  [A loss: 1.157488, acc: 0.132812]\n",
      "309: [D loss: 0.641351, acc: 0.609375]  [A loss: 1.057446, acc: 0.218750]\n",
      "310: [D loss: 0.735437, acc: 0.523438]  [A loss: 1.473978, acc: 0.046875]\n",
      "311: [D loss: 0.630498, acc: 0.648438]  [A loss: 0.918465, acc: 0.257812]\n",
      "312: [D loss: 0.693409, acc: 0.574219]  [A loss: 1.554220, acc: 0.054688]\n",
      "313: [D loss: 0.623058, acc: 0.652344]  [A loss: 0.999154, acc: 0.210938]\n",
      "314: [D loss: 0.684326, acc: 0.578125]  [A loss: 1.540786, acc: 0.023438]\n",
      "315: [D loss: 0.637447, acc: 0.667969]  [A loss: 0.801833, acc: 0.421875]\n",
      "316: [D loss: 0.710364, acc: 0.578125]  [A loss: 1.766374, acc: 0.007812]\n",
      "317: [D loss: 0.549647, acc: 0.761719]  [A loss: 1.048697, acc: 0.140625]\n",
      "318: [D loss: 0.655777, acc: 0.558594]  [A loss: 1.619095, acc: 0.000000]\n",
      "319: [D loss: 0.539307, acc: 0.742188]  [A loss: 1.202817, acc: 0.109375]\n",
      "320: [D loss: 0.628703, acc: 0.593750]  [A loss: 1.614239, acc: 0.000000]\n",
      "321: [D loss: 0.519727, acc: 0.765625]  [A loss: 1.058659, acc: 0.234375]\n",
      "322: [D loss: 0.564467, acc: 0.652344]  [A loss: 1.684732, acc: 0.007812]\n",
      "323: [D loss: 0.420195, acc: 0.847656]  [A loss: 1.052242, acc: 0.250000]\n",
      "324: [D loss: 0.617017, acc: 0.617188]  [A loss: 1.941455, acc: 0.000000]\n",
      "325: [D loss: 0.456479, acc: 0.828125]  [A loss: 1.101069, acc: 0.117188]\n",
      "326: [D loss: 0.519017, acc: 0.667969]  [A loss: 1.864941, acc: 0.000000]\n",
      "327: [D loss: 0.464396, acc: 0.832031]  [A loss: 1.136681, acc: 0.125000]\n",
      "328: [D loss: 0.484227, acc: 0.691406]  [A loss: 1.819640, acc: 0.000000]\n",
      "329: [D loss: 0.419498, acc: 0.863281]  [A loss: 1.380770, acc: 0.015625]\n",
      "330: [D loss: 0.416521, acc: 0.835938]  [A loss: 1.606866, acc: 0.000000]\n",
      "331: [D loss: 0.376890, acc: 0.890625]  [A loss: 1.533173, acc: 0.015625]\n",
      "332: [D loss: 0.390727, acc: 0.835938]  [A loss: 1.763748, acc: 0.000000]\n",
      "333: [D loss: 0.388220, acc: 0.839844]  [A loss: 1.822219, acc: 0.000000]\n",
      "334: [D loss: 0.412727, acc: 0.847656]  [A loss: 1.711503, acc: 0.007812]\n",
      "335: [D loss: 0.818508, acc: 0.496094]  [A loss: 2.946660, acc: 0.000000]\n",
      "336: [D loss: 0.748426, acc: 0.593750]  [A loss: 0.912889, acc: 0.359375]\n",
      "337: [D loss: 1.239736, acc: 0.414062]  [A loss: 1.763607, acc: 0.007812]\n",
      "338: [D loss: 0.706263, acc: 0.613281]  [A loss: 0.711159, acc: 0.546875]\n",
      "339: [D loss: 1.005132, acc: 0.460938]  [A loss: 1.926745, acc: 0.023438]\n",
      "340: [D loss: 0.780380, acc: 0.519531]  [A loss: 1.083748, acc: 0.156250]\n",
      "341: [D loss: 0.778822, acc: 0.484375]  [A loss: 1.327084, acc: 0.078125]\n",
      "342: [D loss: 0.754932, acc: 0.531250]  [A loss: 1.337914, acc: 0.117188]\n",
      "343: [D loss: 0.948761, acc: 0.378906]  [A loss: 1.832789, acc: 0.023438]\n",
      "344: [D loss: 1.013206, acc: 0.316406]  [A loss: 2.213250, acc: 0.000000]\n",
      "345: [D loss: 0.825944, acc: 0.441406]  [A loss: 2.334789, acc: 0.000000]\n",
      "346: [D loss: 0.723113, acc: 0.554688]  [A loss: 1.935962, acc: 0.007812]\n",
      "347: [D loss: 0.714014, acc: 0.574219]  [A loss: 2.848938, acc: 0.000000]\n",
      "348: [D loss: 0.578564, acc: 0.703125]  [A loss: 1.518257, acc: 0.039062]\n",
      "349: [D loss: 0.588025, acc: 0.660156]  [A loss: 2.489985, acc: 0.000000]\n",
      "350: [D loss: 0.461768, acc: 0.796875]  [A loss: 1.611198, acc: 0.062500]\n",
      "351: [D loss: 0.482083, acc: 0.777344]  [A loss: 2.244663, acc: 0.000000]\n",
      "352: [D loss: 0.369394, acc: 0.894531]  [A loss: 1.537121, acc: 0.007812]\n",
      "353: [D loss: 0.408004, acc: 0.804688]  [A loss: 2.063852, acc: 0.000000]\n",
      "354: [D loss: 0.428754, acc: 0.839844]  [A loss: 1.719589, acc: 0.007812]\n",
      "355: [D loss: 0.542280, acc: 0.691406]  [A loss: 1.773700, acc: 0.000000]\n",
      "356: [D loss: 0.540165, acc: 0.742188]  [A loss: 1.160642, acc: 0.148438]\n",
      "357: [D loss: 0.663760, acc: 0.625000]  [A loss: 2.330220, acc: 0.000000]\n",
      "358: [D loss: 0.570727, acc: 0.742188]  [A loss: 1.043232, acc: 0.250000]\n",
      "359: [D loss: 0.842873, acc: 0.503906]  [A loss: 2.803868, acc: 0.000000]\n",
      "360: [D loss: 0.568588, acc: 0.734375]  [A loss: 0.887866, acc: 0.281250]\n",
      "361: [D loss: 0.662764, acc: 0.570312]  [A loss: 2.096226, acc: 0.000000]\n",
      "362: [D loss: 0.509706, acc: 0.773438]  [A loss: 1.258608, acc: 0.101562]\n",
      "363: [D loss: 0.569973, acc: 0.648438]  [A loss: 1.889265, acc: 0.007812]\n",
      "364: [D loss: 0.490253, acc: 0.792969]  [A loss: 1.445750, acc: 0.062500]\n",
      "365: [D loss: 0.508521, acc: 0.726562]  [A loss: 2.257715, acc: 0.000000]\n",
      "366: [D loss: 0.408692, acc: 0.839844]  [A loss: 1.328324, acc: 0.039062]\n",
      "367: [D loss: 0.471210, acc: 0.742188]  [A loss: 1.973352, acc: 0.007812]\n",
      "368: [D loss: 0.387228, acc: 0.875000]  [A loss: 1.419670, acc: 0.015625]\n",
      "369: [D loss: 0.461335, acc: 0.722656]  [A loss: 2.032688, acc: 0.000000]\n",
      "370: [D loss: 0.356228, acc: 0.933594]  [A loss: 1.356989, acc: 0.039062]\n",
      "371: [D loss: 0.378970, acc: 0.851562]  [A loss: 1.735291, acc: 0.015625]\n",
      "372: [D loss: 0.401135, acc: 0.886719]  [A loss: 1.626399, acc: 0.023438]\n",
      "373: [D loss: 0.708567, acc: 0.523438]  [A loss: 2.113703, acc: 0.007812]\n",
      "374: [D loss: 0.783931, acc: 0.515625]  [A loss: 1.648982, acc: 0.015625]\n",
      "375: [D loss: 0.937142, acc: 0.378906]  [A loss: 1.879970, acc: 0.046875]\n",
      "376: [D loss: 0.957320, acc: 0.378906]  [A loss: 1.412800, acc: 0.109375]\n",
      "377: [D loss: 1.087649, acc: 0.285156]  [A loss: 1.393966, acc: 0.078125]\n",
      "378: [D loss: 0.973313, acc: 0.378906]  [A loss: 0.848943, acc: 0.398438]\n",
      "379: [D loss: 0.896200, acc: 0.402344]  [A loss: 1.221042, acc: 0.093750]\n",
      "380: [D loss: 0.930539, acc: 0.406250]  [A loss: 1.181228, acc: 0.171875]\n",
      "381: [D loss: 0.828840, acc: 0.472656]  [A loss: 1.250995, acc: 0.093750]\n",
      "382: [D loss: 0.767154, acc: 0.480469]  [A loss: 1.383652, acc: 0.062500]\n",
      "383: [D loss: 0.729535, acc: 0.542969]  [A loss: 1.161028, acc: 0.132812]\n",
      "384: [D loss: 0.739570, acc: 0.527344]  [A loss: 1.247168, acc: 0.085938]\n",
      "385: [D loss: 0.690632, acc: 0.585938]  [A loss: 1.418995, acc: 0.070312]\n",
      "386: [D loss: 0.681801, acc: 0.601562]  [A loss: 1.576790, acc: 0.046875]\n",
      "387: [D loss: 0.676705, acc: 0.625000]  [A loss: 1.118597, acc: 0.226562]\n",
      "388: [D loss: 0.823134, acc: 0.480469]  [A loss: 2.210863, acc: 0.007812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389: [D loss: 1.001844, acc: 0.414062]  [A loss: 1.204905, acc: 0.242188]\n",
      "390: [D loss: 1.199719, acc: 0.292969]  [A loss: 2.179393, acc: 0.000000]\n",
      "391: [D loss: 1.474732, acc: 0.191406]  [A loss: 2.687987, acc: 0.000000]\n",
      "392: [D loss: 1.042980, acc: 0.414062]  [A loss: 0.625797, acc: 0.648438]\n",
      "393: [D loss: 1.220279, acc: 0.378906]  [A loss: 2.704173, acc: 0.000000]\n",
      "394: [D loss: 0.911170, acc: 0.476562]  [A loss: 0.950143, acc: 0.343750]\n",
      "395: [D loss: 1.030252, acc: 0.351562]  [A loss: 1.612896, acc: 0.046875]\n",
      "396: [D loss: 0.781822, acc: 0.511719]  [A loss: 1.508896, acc: 0.062500]\n",
      "397: [D loss: 0.862705, acc: 0.414062]  [A loss: 1.394405, acc: 0.070312]\n",
      "398: [D loss: 0.793670, acc: 0.460938]  [A loss: 1.371515, acc: 0.085938]\n",
      "399: [D loss: 0.776325, acc: 0.507812]  [A loss: 1.351326, acc: 0.046875]\n",
      "400: [D loss: 0.703597, acc: 0.613281]  [A loss: 1.359116, acc: 0.031250]\n",
      "401: [D loss: 0.702998, acc: 0.574219]  [A loss: 1.417546, acc: 0.078125]\n",
      "402: [D loss: 0.646214, acc: 0.636719]  [A loss: 1.321231, acc: 0.054688]\n",
      "403: [D loss: 0.592138, acc: 0.679688]  [A loss: 1.631897, acc: 0.015625]\n",
      "404: [D loss: 0.617961, acc: 0.691406]  [A loss: 1.124658, acc: 0.093750]\n",
      "405: [D loss: 0.632405, acc: 0.613281]  [A loss: 1.871115, acc: 0.000000]\n",
      "406: [D loss: 0.547648, acc: 0.730469]  [A loss: 1.246534, acc: 0.085938]\n",
      "407: [D loss: 0.542018, acc: 0.675781]  [A loss: 1.933149, acc: 0.015625]\n",
      "408: [D loss: 0.479612, acc: 0.820312]  [A loss: 1.097436, acc: 0.156250]\n",
      "409: [D loss: 0.503418, acc: 0.730469]  [A loss: 1.982181, acc: 0.000000]\n",
      "410: [D loss: 0.529659, acc: 0.738281]  [A loss: 1.703520, acc: 0.007812]\n",
      "411: [D loss: 0.605439, acc: 0.679688]  [A loss: 1.609634, acc: 0.007812]\n",
      "412: [D loss: 0.557483, acc: 0.722656]  [A loss: 1.437858, acc: 0.046875]\n",
      "413: [D loss: 0.621862, acc: 0.660156]  [A loss: 1.515766, acc: 0.000000]\n",
      "414: [D loss: 0.669278, acc: 0.562500]  [A loss: 1.761879, acc: 0.007812]\n",
      "415: [D loss: 0.666894, acc: 0.644531]  [A loss: 1.031081, acc: 0.148438]\n",
      "416: [D loss: 0.724646, acc: 0.531250]  [A loss: 1.939011, acc: 0.000000]\n",
      "417: [D loss: 0.781085, acc: 0.570312]  [A loss: 0.952761, acc: 0.242188]\n",
      "418: [D loss: 0.894291, acc: 0.464844]  [A loss: 2.280422, acc: 0.000000]\n",
      "419: [D loss: 0.913577, acc: 0.437500]  [A loss: 1.061264, acc: 0.171875]\n",
      "420: [D loss: 1.179314, acc: 0.324219]  [A loss: 1.628929, acc: 0.031250]\n",
      "421: [D loss: 0.961111, acc: 0.382812]  [A loss: 0.967893, acc: 0.242188]\n",
      "422: [D loss: 1.060025, acc: 0.312500]  [A loss: 1.392496, acc: 0.109375]\n",
      "423: [D loss: 1.017469, acc: 0.300781]  [A loss: 1.228982, acc: 0.132812]\n",
      "424: [D loss: 0.688945, acc: 0.609375]  [A loss: 1.028009, acc: 0.250000]\n",
      "425: [D loss: 0.869999, acc: 0.414062]  [A loss: 1.873725, acc: 0.000000]\n",
      "426: [D loss: 0.611936, acc: 0.734375]  [A loss: 0.800595, acc: 0.398438]\n",
      "427: [D loss: 0.931431, acc: 0.445312]  [A loss: 1.972272, acc: 0.007812]\n",
      "428: [D loss: 0.690699, acc: 0.597656]  [A loss: 1.134071, acc: 0.156250]\n",
      "429: [D loss: 0.749898, acc: 0.550781]  [A loss: 1.976482, acc: 0.000000]\n",
      "430: [D loss: 0.719985, acc: 0.578125]  [A loss: 1.344874, acc: 0.109375]\n",
      "431: [D loss: 0.785747, acc: 0.519531]  [A loss: 2.314490, acc: 0.000000]\n",
      "432: [D loss: 0.667732, acc: 0.589844]  [A loss: 1.730660, acc: 0.015625]\n",
      "433: [D loss: 0.781713, acc: 0.519531]  [A loss: 3.050345, acc: 0.000000]\n",
      "434: [D loss: 0.509625, acc: 0.750000]  [A loss: 0.706261, acc: 0.570312]\n",
      "435: [D loss: 1.194440, acc: 0.476562]  [A loss: 2.316400, acc: 0.000000]\n",
      "436: [D loss: 0.839335, acc: 0.457031]  [A loss: 1.044205, acc: 0.320312]\n",
      "437: [D loss: 0.858459, acc: 0.441406]  [A loss: 1.256232, acc: 0.156250]\n",
      "438: [D loss: 0.793000, acc: 0.488281]  [A loss: 1.407361, acc: 0.109375]\n",
      "439: [D loss: 0.785116, acc: 0.496094]  [A loss: 1.263212, acc: 0.109375]\n",
      "440: [D loss: 0.693492, acc: 0.582031]  [A loss: 1.304568, acc: 0.093750]\n",
      "441: [D loss: 0.690926, acc: 0.582031]  [A loss: 1.560414, acc: 0.054688]\n",
      "442: [D loss: 0.592324, acc: 0.648438]  [A loss: 1.361301, acc: 0.046875]\n",
      "443: [D loss: 0.602999, acc: 0.667969]  [A loss: 1.622450, acc: 0.046875]\n",
      "444: [D loss: 0.557408, acc: 0.691406]  [A loss: 1.286554, acc: 0.078125]\n",
      "445: [D loss: 0.611139, acc: 0.652344]  [A loss: 2.062851, acc: 0.000000]\n",
      "446: [D loss: 0.493772, acc: 0.789062]  [A loss: 1.131433, acc: 0.156250]\n",
      "447: [D loss: 0.578335, acc: 0.640625]  [A loss: 1.941182, acc: 0.000000]\n",
      "448: [D loss: 0.422199, acc: 0.882812]  [A loss: 1.447177, acc: 0.031250]\n",
      "449: [D loss: 0.521023, acc: 0.722656]  [A loss: 2.020432, acc: 0.000000]\n",
      "450: [D loss: 0.477455, acc: 0.789062]  [A loss: 1.367536, acc: 0.039062]\n",
      "451: [D loss: 0.858567, acc: 0.496094]  [A loss: 3.117384, acc: 0.000000]\n",
      "452: [D loss: 0.475459, acc: 0.785156]  [A loss: 1.005877, acc: 0.250000]\n",
      "453: [D loss: 0.887893, acc: 0.488281]  [A loss: 1.611299, acc: 0.023438]\n",
      "454: [D loss: 0.545361, acc: 0.757812]  [A loss: 0.968654, acc: 0.210938]\n",
      "455: [D loss: 0.724002, acc: 0.539062]  [A loss: 1.433549, acc: 0.031250]\n",
      "456: [D loss: 0.652394, acc: 0.644531]  [A loss: 1.174842, acc: 0.125000]\n",
      "457: [D loss: 0.602930, acc: 0.628906]  [A loss: 1.493343, acc: 0.039062]\n",
      "458: [D loss: 0.573890, acc: 0.691406]  [A loss: 1.360147, acc: 0.054688]\n",
      "459: [D loss: 0.539663, acc: 0.710938]  [A loss: 1.367781, acc: 0.070312]\n",
      "460: [D loss: 0.562817, acc: 0.707031]  [A loss: 1.461273, acc: 0.023438]\n",
      "461: [D loss: 0.553264, acc: 0.695312]  [A loss: 1.689942, acc: 0.046875]\n",
      "462: [D loss: 0.537363, acc: 0.714844]  [A loss: 1.709876, acc: 0.062500]\n",
      "463: [D loss: 0.518585, acc: 0.761719]  [A loss: 1.374985, acc: 0.164062]\n",
      "464: [D loss: 0.557714, acc: 0.687500]  [A loss: 1.382022, acc: 0.101562]\n",
      "465: [D loss: 0.517442, acc: 0.761719]  [A loss: 1.485743, acc: 0.070312]\n",
      "466: [D loss: 0.561723, acc: 0.675781]  [A loss: 1.895741, acc: 0.023438]\n",
      "467: [D loss: 0.426113, acc: 0.824219]  [A loss: 1.310776, acc: 0.070312]\n",
      "468: [D loss: 0.533287, acc: 0.691406]  [A loss: 2.200625, acc: 0.007812]\n",
      "469: [D loss: 0.438603, acc: 0.835938]  [A loss: 1.278988, acc: 0.085938]\n",
      "470: [D loss: 0.514431, acc: 0.695312]  [A loss: 2.087720, acc: 0.000000]\n",
      "471: [D loss: 0.697244, acc: 0.605469]  [A loss: 1.724451, acc: 0.015625]\n",
      "472: [D loss: 0.877338, acc: 0.421875]  [A loss: 2.708913, acc: 0.000000]\n",
      "473: [D loss: 0.625303, acc: 0.699219]  [A loss: 0.679117, acc: 0.578125]\n",
      "474: [D loss: 0.973184, acc: 0.492188]  [A loss: 3.184115, acc: 0.000000]\n",
      "475: [D loss: 0.584333, acc: 0.714844]  [A loss: 0.965114, acc: 0.273438]\n",
      "476: [D loss: 0.967233, acc: 0.464844]  [A loss: 2.178720, acc: 0.007812]\n",
      "477: [D loss: 0.631616, acc: 0.644531]  [A loss: 1.305503, acc: 0.140625]\n",
      "478: [D loss: 0.887255, acc: 0.464844]  [A loss: 2.364563, acc: 0.007812]\n",
      "479: [D loss: 0.672012, acc: 0.593750]  [A loss: 1.190759, acc: 0.218750]\n",
      "480: [D loss: 0.796809, acc: 0.480469]  [A loss: 1.993297, acc: 0.007812]\n",
      "481: [D loss: 0.720665, acc: 0.597656]  [A loss: 1.011539, acc: 0.257812]\n",
      "482: [D loss: 0.946841, acc: 0.421875]  [A loss: 1.860362, acc: 0.015625]\n",
      "483: [D loss: 0.725732, acc: 0.585938]  [A loss: 1.098566, acc: 0.179688]\n",
      "484: [D loss: 0.916158, acc: 0.417969]  [A loss: 2.354373, acc: 0.007812]\n",
      "485: [D loss: 0.660928, acc: 0.656250]  [A loss: 1.058167, acc: 0.234375]\n",
      "486: [D loss: 0.872762, acc: 0.433594]  [A loss: 1.778719, acc: 0.015625]\n",
      "487: [D loss: 0.670322, acc: 0.632812]  [A loss: 1.025720, acc: 0.234375]\n",
      "488: [D loss: 0.700538, acc: 0.578125]  [A loss: 1.798167, acc: 0.015625]\n",
      "489: [D loss: 0.574731, acc: 0.703125]  [A loss: 1.396191, acc: 0.078125]\n",
      "490: [D loss: 0.651372, acc: 0.621094]  [A loss: 1.845672, acc: 0.007812]\n",
      "491: [D loss: 0.536076, acc: 0.753906]  [A loss: 1.357739, acc: 0.078125]\n",
      "492: [D loss: 0.523153, acc: 0.714844]  [A loss: 1.859517, acc: 0.000000]\n",
      "493: [D loss: 0.518370, acc: 0.765625]  [A loss: 1.146827, acc: 0.203125]\n",
      "494: [D loss: 0.557054, acc: 0.648438]  [A loss: 1.843152, acc: 0.007812]\n",
      "495: [D loss: 0.524432, acc: 0.726562]  [A loss: 1.008792, acc: 0.312500]\n",
      "496: [D loss: 0.606487, acc: 0.640625]  [A loss: 1.681140, acc: 0.046875]\n",
      "497: [D loss: 0.426459, acc: 0.839844]  [A loss: 1.100410, acc: 0.187500]\n",
      "498: [D loss: 0.594637, acc: 0.621094]  [A loss: 1.657791, acc: 0.023438]\n",
      "499: [D loss: 0.306050, acc: 0.894531]  [A loss: 1.023645, acc: 0.171875]\n",
      "Elapsed: 11.885339216391246 min \n"
     ]
    }
   ],
   "source": [
    "GENERATED = []\n",
    "\n",
    "class ElapsedTimer(object):\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "    def elapsed(self,sec):\n",
    "        if sec < 60:\n",
    "            return str(sec) + \" sec\"\n",
    "        elif sec < (60 * 60):\n",
    "            return str(sec / 60) + \" min\"\n",
    "        else:\n",
    "            return str(sec / (60 * 60)) + \" hr\"\n",
    "    def elapsed_time(self):\n",
    "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time) )\n",
    "\n",
    "class DCGAN(object):\n",
    "    def __init__(self, img_rows=NUMCHARS + 1, img_cols=MAXLEN + 1, channel=1):\n",
    "\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.channel = channel\n",
    "        self.D = None   # discriminator\n",
    "        self.G = None   # generator\n",
    "        self.AM = None  # adversarial model\n",
    "        self.DM = None  # discriminator model\n",
    "\n",
    "\n",
    "    def discriminator(self):\n",
    "        if self.D:\n",
    "            return self.D\n",
    "        self.D = Sequential()\n",
    "        depth = 64\n",
    "        dropout = 0.3\n",
    "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
    "        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,\\\n",
    "            padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "#         self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
    "#         self.D.add(LeakyReLU(alpha=0.2))\n",
    "#         self.D.add(Dropout(dropout))\n",
    "\n",
    "#         self.D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
    "#         self.D.add(LeakyReLU(alpha=0.2))\n",
    "#         self.D.add(Dropout(dropout))\n",
    "\n",
    "        # Out: 1-dim probability\n",
    "        self.D.add(Flatten())\n",
    "        self.D.add(Dense(1))\n",
    "        self.D.add(Activation('sigmoid'))\n",
    "        self.D.summary()\n",
    "        return self.D\n",
    "\n",
    "    def generator(self):\n",
    "        if self.G:\n",
    "            return self.G\n",
    "        self.G = Sequential()\n",
    "        dropout = 0.3\n",
    "        depth = 64+64+64+64\n",
    "        dim_1 = int(np.ceil(NUMCHARS / 4))\n",
    "        dim_2 = int(np.ceil(MAXLEN / 4))\n",
    "        # In: 100\n",
    "        # Out: dim x dim x depth\n",
    "        self.G.add(Dense(dim_1*dim_2*depth, input_dim=100))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "        self.G.add(Reshape((dim_1, dim_2, depth)))\n",
    "        self.G.add(Dropout(dropout))\n",
    "\n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(Conv2DTranspose(1, 5, padding='same'))\n",
    "        self.G.add(Activation('sigmoid'))\n",
    "        self.G.summary()\n",
    "        return self.G\n",
    "\n",
    "    def discriminator_model(self):\n",
    "        if self.DM:\n",
    "            return self.DM\n",
    "        optimizer = RMSprop(lr=0.0008, clipvalue=1.0, decay=6e-8)\n",
    "        self.DM = Sequential()\n",
    "        self.DM.add(self.discriminator())\n",
    "        self.DM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
    "            metrics=['accuracy'])\n",
    "        return self.DM\n",
    "\n",
    "    def adversarial_model(self):\n",
    "        if self.AM:\n",
    "            return self.AM\n",
    "        # change to adam?\n",
    "        optimizer = RMSprop(lr=0.0004, clipvalue=1.0, decay=3e-8)\n",
    "        self.AM = Sequential()\n",
    "        self.AM.add(self.generator())\n",
    "        self.AM.add(self.discriminator())\n",
    "        self.AM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
    "            metrics=['accuracy'])\n",
    "        return self.AM\n",
    "\n",
    "class MNIST_DCGAN(object):\n",
    "    def __init__(self):\n",
    "        self.img_rows = NUMCHARS + 1\n",
    "        self.img_cols = MAXLEN + 1\n",
    "        self.channel = 1\n",
    "        self.disc_losses = []\n",
    "        self.gen_losses = []\n",
    "\n",
    "        X = []\n",
    "        data = np.load(\"alkanes.npy\")[:2048]\n",
    "        for line in data:\n",
    "            new = np.reshape(line, (MAXLEN, NUMCHARS)).T\n",
    "            new = np.vstack((new, np.zeros(MAXLEN)))\n",
    "            new = np.hstack((new, np.zeros((NUMCHARS + 1, 1))))\n",
    "            X.append(smooth(new, epsilon))\n",
    "        self.x_train = np.array(X).reshape(-1, NUMCHARS + 1, MAXLEN + 1, 1).astype(np.float32)\n",
    "\n",
    "        self.DCGAN = DCGAN()\n",
    "        self.discriminator =  self.DCGAN.discriminator_model()\n",
    "        self.adversarial = self.DCGAN.adversarial_model()\n",
    "        self.generator = self.DCGAN.generator()\n",
    "\n",
    "    def train(self, train_steps=1000, batch_size=128, save_interval=0):\n",
    "        noise_input = None\n",
    "        if save_interval>0:\n",
    "            noise_input = np.random.uniform(0.0, 1.0, size=[16, 100])\n",
    "        for i in range(train_steps):\n",
    "            images_train = self.x_train[np.random.randint(0,\n",
    "                self.x_train.shape[0], size=batch_size), :, :, :]\n",
    "            noise = np.random.uniform(0.0, 1.0, size=[batch_size, 100])\n",
    "            images_fake = self.generator.predict(noise)\n",
    "            x = np.concatenate((images_train, images_fake))\n",
    "            \n",
    "            y = np.ones([2*batch_size, 1])\n",
    "            y[batch_size:, :] = 0\n",
    "            d_loss = self.discriminator.train_on_batch(x, y)\n",
    "            \n",
    "#             if i > 0:\n",
    "#                 if a_loss[1] < 0.025:\n",
    "#                     y = np.ones([batch_size, 1])\n",
    "#                     noise = np.random.uniform(0.0, 1.0, size=[batch_size, 100])\n",
    "#                     a_loss = self.adversarial.train_on_batch(noise, y)\n",
    "            y = np.ones([batch_size, 1])\n",
    "            noise = np.random.uniform(0.0, 1.0, size=[batch_size, 100])\n",
    "            a_loss = self.adversarial.train_on_batch(noise, y)\n",
    "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
    "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
    "            self.disc_losses.append(d_loss[0])\n",
    "            self.gen_losses.append(a_loss[0])\n",
    "            print(log_mesg)\n",
    "            if save_interval>0:\n",
    "                if (i+1)%save_interval==0:\n",
    "                    noise = np.random.uniform(0.0, 1.0, size=[10, 100])\n",
    "                    images = self.generator.predict(noise)\n",
    "                    GENERATED.append(images)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mnist_dcgan = MNIST_DCGAN()\n",
    "    timer = ElapsedTimer()\n",
    "    steps = 500\n",
    "    mnist_dcgan.train(train_steps=steps, batch_size=128, save_interval=100)\n",
    "    timer.elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAIABJREFUeJztnXecFPX9/1+fLXd7ld6LB0oHAT2aIGIjiEbUmKixkajEFqNEI2qCJZqYfNWg0Vhi96fR2IlixQaKSG8CCohw1KPccf22fH5/zHxmPzM7uzu7O7Nzt7yfjwcPbndndz6zO/Oa9+fdPoxzDoIgCCK38Lg9AIIgCMJ+SNwJgiByEBJ3giCIHITEnSAIIgchcScIgshBSNwJgiByEBJ3giCIHCSpuDPGejHGPmWMrWeMrWOM/c5km0mMsWrG2Er132xnhksQBEFYwWdhmxCA33POlzPGSgAsY4x9xDn/1rDdAs75GfYPkSAIgkiVpOLOOd8FYJf6dw1jbD2AHgCM4p4SHTt25GVlZZl8BEEQxGHHsmXL9nHOOyXbzorlrsEYKwMwEsBik5fHMcZWAdgJ4EbO+bpEn1VWVoalS5emsnuCIIjDHsbYj1a2syzujLFiAK8DuJ5zfsjw8nIAR3DOaxljUwG8BaCfyWfMADADAHr37m111wRBEESKWMqWYYz5oQj7i5zzN4yvc84Pcc5r1b/nAfAzxjqabPcE57ycc17eqVPSWQVBEASRJlayZRiApwCs55w/EGebrup2YIyNVj93v50DJQiCIKxjxS0zHsDFANYwxlaqz90KoDcAcM4fA3AugKsYYyEADQDO59RLmCAyJhgMoqKiAo2NjW4PhcgygUAAPXv2hN/vT+v9VrJlFgJgSbZ5GMDDaY2AIIi4VFRUoKSkBGVlZVAnx8RhAOcc+/fvR0VFBfr06ZPWZ1CFKkG0YBobG9GhQwcS9sMMxhg6dOiQ0YyNxJ0gWjgk7Icnmf7uJO4u8cV3ldh+oN7tYRAEkaOQuLvEJU9/g+P//qnbwyCIpHi9XowYMQJDhgzB8OHD8cADDyASiQAAli5diuuuuy7jfTz22GN4/vnnU3rPcccdl/b+nn32WezcuTPt9wPAHXfcgfvuuy+jz3CSlCpUCYI4/CgoKMDKlUqi3N69e/HLX/4S1dXVuPPOO1FeXo7y8vKMPj8UCuHKK69M+X1fffVV2vt89tlnMXToUHTv3t3ye8LhMLxeb9r7zDZkuRMEYZnOnTvjiSeewMMPPwzOOT777DOccYbSL/Dzzz/HiBEjMGLECIwcORI1NTUAgL///e8YNmwYhg8fjlmzZgEAJk2ahFtvvRUnnHACHnzwQZ0VPGnSJNxwww2YOHEiBg0ahCVLluCcc85Bv3798Mc//lEbS3FxMQDgs88+w6RJk3Duuedi4MCBuPDCCyEyse+66y6MGjUKQ4cOxYwZM8A5x2uvvYalS5fiwgsvxIgRI9DQ0ID58+dj5MiRGDZsGH7961+jqakJgNIm5a677sKECRPw6quvWvqOHnjgAQwdOhRDhw7FnDlzAAB1dXU4/fTTMXz4cAwdOhSvvPIKAGDWrFkYPHgwjj76aNx4440Z/TZGyHIniFbCnf9bh293Gjt/ZMbg7qW4/adDUnpP3759EYlEsHfvXt3z9913Hx555BGMHz8etbW1CAQCeO+99/DWW29h8eLFKCwsxIEDB7Ttq6qq8PnnnwNQXBwyeXl5+OKLL/Dggw9i2rRpWLZsGdq3b48jjzwSN9xwAzp06KDbfsWKFVi3bh26d++O8ePH48svv8SECRNw7bXXYvZspQP5xRdfjHfeeQfnnnsuHn74Ydx3330oLy9HY2Mjpk+fjvnz56N///645JJL8Oijj+L6668HoOSbL1y40NJ3s2zZMjzzzDNYvHgxOOcYM2YMTjjhBGzZsgXdu3fHu+++CwCorq7GgQMH8Oabb2LDhg1gjKGqqsr6j2ABstwJgkgZsxrF8ePHY+bMmXjooYdQVVUFn8+Hjz/+GL/61a9QWFgIAGjfvr22/XnnnRf3888880wAwLBhwzBkyBB069YN+fn56Nu3L7Zv3x6z/ejRo9GzZ094PB6MGDECW7duBQB8+umnGDNmDIYNG4ZPPvkE69bF9jPcuHEj+vTpg/79+wMALr30UnzxxReWxmlk4cKFOPvss1FUVITi4mKcc845WLBgAYYNG4aPP/4YN998MxYsWIA2bdqgtLQUgUAAl19+Od544w3tO7ILstwJopWQqoXtFFu2bIHX60Xnzp2xfv167flZs2bh9NNPx7x58zB27Fh8/PHH4JzHTekrKiqKu4/8/HwAgMfj0f4Wj0OhUNztASUAHAqF0NjYiKuvvhpLly5Fr169cMcdd5jmjScrpk80Tquf1b9/fyxbtgzz5s3DLbfcgsmTJ2P27Nn45ptvMH/+fLz88st4+OGH8cknn1jeVzLIcicIwjKVlZW48sorce2118aI9ubNmzFs2DDcfPPNKC8vx4YNGzB58mQ8/fTTqK9X0n5lt4zTCCHv2LEjamtr8dprr2mvlZSUaDGBgQMHYuvWrdi0aRMA4IUXXsAJJ5yQ1j4nTpyIt956C/X19airq8Obb76J448/Hjt37kRhYSEuuugi3HjjjVi+fDlqa2tRXV2NqVOnYs6cOVrQ2i7IcicIIiENDQ0YMWIEgsEgfD4fLr74YsycOTNmuzlz5uDTTz+F1+vF4MGDcdpppyE/Px8rV65EeXk58vLyMHXqVPzlL3/Jyrjbtm2LK664AsOGDUNZWRlGjRqlvTZ9+nRceeWVKCgowKJFi/DMM8/g5z//OUKhEEaNGmU5e+fuu+/WgqaA0i5i+vTpGD16NADg8ssvx8iRI/HBBx/gpptugsfjgd/vx6OPPoqamhpMmzYNjY2N4JzjH//4h63Hz9zq71VeXs4P58U6ymYpgZWt957u8kiIlsz69esxaNAgt4dBuITZ788YW8Y5T5p/Sm4ZgiCIHITEnSAIIgchcSeIFg4tjXB4kunvTuJOEC2YQCCA/fv3k8AfZoh+7oFAIO3PoGwZgmjB9OzZExUVFaisrHR7KESWESsxpQuJO0G0YPx+f9or8RCHN+SWIQiCyEFI3F2GfKkEQTgBibvLBMMk7gRB2A+Ju8sEwxG3h0AQRA5C4u4yzSESd4Ig7IfE3WXIcicIwglI3F2miSx3giAcgMTdBeQMmWay3AmCcAASdxeQsx+bgiTuBEHYD4m7C8jJj7uqG1wbB0EQuQuJu8v8sK/O7SEQBJGDkLi7gOxz30LiThCEA5C4u4DslvmhksSdIAj7IXF3ATmgWtMUdG8gBEHkLCTuLsAl2z1CyTIEQTgAibsLyJZ7hLpCEgThACTuLhOOkLgTBGE/JO4uQ5Y7QRBOQOLuAnq3jHvjIAgid0kq7oyxXoyxTxlj6xlj6xhjvzPZhjHGHmKMbWKMrWaMHePMcHMDOaBKbhmCIJzAygLZIQC/55wvZ4yVAFjGGPuIc/6ttM1pAPqp/8YAeFT9nzBBttxJ3AmCcIKkljvnfBfnfLn6dw2A9QB6GDabBuB5rvA1gLaMsW62jzZHkOWc1lAlCMIJUvK5M8bKAIwEsNjwUg8A26XHFYi9ARAqsqCHSdwJgnAAy+LOGCsG8DqA6znnh4wvm7wlRrUYYzMYY0sZY0srKytTG2mOQu3cCYJwAkvizhjzQxH2Fznnb5hsUgGgl/S4J4Cdxo04509wzss55+WdOnVKZ7w5gXzXo1RIgiCcwEq2DAPwFID1nPMH4mw2F8AlatbMWADVnPNdNo4zp6AKVYIgnMZKtsx4ABcDWMMYW6k+dyuA3gDAOX8MwDwAUwFsAlAP4Ff2DzWHoGwZgiAcJqm4c84XwtynLm/DAVxj16ByHZHn7vUwREjcCYJwAKpQdQHhifEyRhWqBEE4Aom7Cwg993oYpUISBOEIJO4uQm4ZgiCcgsTdBUQRk4dRERNBEM5A4u4CQs59Xg84pxYEBEHYD4m7C2gBVY+ShESeGYIg7IbE3QVEKqRPFXfKdScIwm5I3N1A1XIPE5Y7iTtBEPZC4u4CUZ87iTtBEM5A4u4iXkZuGYIgnIHE3QViAqrU9pcgCJshcXcBubcMQG4ZgiDsh8TdBYyWOxUyEQRhNyTuLqAFVDW3DIk7QRD2QuLuAlr7AbLcCYJwCBJ3F/EyqlAlCMIZSNxdIDZbhtSdIAh7IXF3EVHERHnuBEHYDYm7C3BqP0AQhMOQuLuAsXEYiTtBEHZD4u4CUZ+78vWHqUKVIAibIXF3AWOeO/ncCYKwGxJ3F6H2AwRBOAWJuwsYi5hI3AmCsBsSdxcgtwxBEE5D4u4ClApJEITTkLi7gjEV0s2xEASRi5C4u4CWCkkVqgRBOASJuwtQy1+CIJyGxN1FPNQVkiAIhyBxdwFaiYkgCKchcXeBmN4yZLoTBGEzJO4uoKVCUp47QRAOQeLuAkLcqSskQRBOQeLuAsItQ71l3Ke6IYiyWe/i42/3uD0UgrAVEncXMFru1PLXPb7fUwMA+Ndnm1weCUHYC4m7i1DjMPdRs1EJIudIKu6MsacZY3sZY2vjvD6JMVbNGFup/ptt/zBzEy/1liEIwiF8FrZ5FsDDAJ5PsM0CzvkZtozoMCAmz52yZQiCsJmkljvn/AsAB7IwlsMGY547iTtBEHZjl899HGNsFWPsPcbYEJs+M2cxWu7klXEf+gmIXMOKWyYZywEcwTmvZYxNBfAWgH5mGzLGZgCYAQC9e/e2YdetEyEk2gLZpO4uQhFVIjfJ2HLnnB/inNeqf88D4GeMdYyz7ROc83LOeXmnTp0y3XWrR7hlQuSWIQ4Dlm49gE17a90exmFDxuLOGOvKmJL2wRgbrX7m/kw/N5cRa6j6faq4U6I7cRhw7mOLcMoDn7s9jMOGpG4Zxth/AEwC0JExVgHgdgB+AOCcPwbgXABXMcZCABoAnM85+RkSIb6cPK8XABAkcXcdOmOJXCOpuHPOL0jy+sNQUiUJiwgh8asrMQXDpCxuQUVMRK5CFaquINwyytdPljtBEHZD4u4CWiokY/B6GEI5bLn/+4st+HbnIbeHQRCHHXakQhIpIqScMSVjJpct93vmrQcAbL33dJdHQhCHF2S5uwgDQ57Xk7M+99YUV289I22dtKZzIVcgcXcB+Tz3eRlCkdy03FtDWwWKp2YHquXIPiTuLiCsGMYAv9eTs26Z1lB5q42wFYy1NdMcys1zvCVD4u4Cms8dQtxzU1hag+VOmp4dctWAacmQuLsAl9Td783dgGrrEPeWP8ZcoDlHz/GWDIm7C4iWvwwMPq8nZ1MhW0MoITe/+ZZHrs5OWzIk7m6gnufC556rVk1rCBRHWsHsIhcIks8965C4u4jic2c52zisNQVUV1VU44d9da6OJZfJVddjS4bE3QVkycvlgGorMNx169eeeN9n7g0kx2kiyz3rkLi7ANfcMiynK1Rbg1umJTjda5tCqG8OuT0MR8nVc7wlQ+LuAlpAlQF5vtzNc28N2t4SXO5Db/8A5Xd/7PYwHCVXZ6ctGRJ3F9Asdyi9ZXK1eq81+NwjLWSM9c1ht4fgKLlqwLRkSNxdQG4c5vd6crZ6L5ym6b54y378c/73No/GnJYh7blPrmaEtWSoK6SrMPi9nty13NO8ns974msAwG9PNl1n3VZaiuWe6+SqAdOSIcvdBeSqSKpQdZlWMMRcIFfP8ZYMibsL6Pq553CFamsQ98PFcp/5ykrc/vZa1/ZP4p59SNzdQAqo5nKFaqYB1Wz0fTlMtB1vrNiB5xb96Nr+g6HoF52rRXstDRJ3F4imQrLcrlDN0HLPRvrc4WK5u41swBx123sujuTwgcTdIsFwBDuqGmz5LG6w3MktY042iqBy85tveZBbJvuQuFtk9ttrMf7eT1DdEMz4s6IVqspKTDnrlpHEPZ0GXdmw3Knlb3bIZrYM5xx/nbceqyuqsrbPlgiJu0U+21gJAKhrsq9MXKyhmrupkNHjSucGlg13FWl7dmgM2vdbLvvxQMJzIxzhePyLLTjz4S9t22drhMTdImKtTTt8tMbGYeEIbxWZJakiB1Q37q5J+f3ZuOnl4NfeImkM2VOBu3J7FX726CI8mKDIjX5SBRJ3izCmyLsdlp68hmphnheA0jwq15BdMdMeSd2KyoaflgKq2aExaI+4V9Y0AQDW7zoUd5tcNJTSgcTdIh71m7Lbci8N+AEANY2Z+/JbGpla3tkINJMMZAejWybdRVI86hQ60WVI92sFEneLMNUxY4erQA6olgSUDhA1jblnubeKbBlSgqzQZLDcg2n+tuoEOqGR1Roa1mUDEneLCIvBHldBdA3VEs1yzz1xN16Aw27/IKX3ZydbxvFdEIj1uac7KxNGVqJ3k6tNgcTdIh7VZLDDVWBuueeuW6Z/l2IAQE2KcYVsuGVICLKD0S2TqZGUaFJI6+IqkLhbxKOa7nYG+XLdLSMusoDfm9b70526p4JR28lN4wzGgGq6szJx/SX6nUjbFUjcLSJSIW3xuUt/l+RwQDWcobi7YblnwxX0wtc/Ynd1o+P7aUnEint6N+5mTdzjb0PZMgok7hYRbhk7LPdo+wGmWe6HctByNxP3VKbMWSliMjx2Ov1yd3Uj/vTWWvz62SXK/nNkprDw+30YPPt9HIpjpBjdMuneuDXLPYHXPVe+00whcbeIiNLb4nOX1lAN+L3I83py0i0jshYCvuhplsrMJ5gFC8woBE7PFsRM4WB9s/rY0d1ljQfnf4f65jA27DIvVjMGVNNtuSG6Sya03EncAZC4W0YLqNrgB5YbhwGK3z2X3TIFed6Y5xLh0W6k2fe5b9yTeiVtKojzSIh8rrgQRBZLvAB1k9FyT/M6alLPiZXbq7SCJiM58pVmDIm7RZiWCmmfz118ZmG+NycXSNbcMr6ouFu5qKMusOy3H/jF44sc3V80T1v8n4VjzILaieNqitMgLMbnHkrTLaN+fn1zGKc9+IXpNpQto0DibhF7UyE1eQegiF8urjEZ9blHTzMrBpuds6RkuJUKKXabaedMK2SjR4/4zeLNQBuDYXjFlAzpZ0LJ7px9tc2m21B6q0JScWeMPc0Y28sYM12jiyk8xBjbxBhbzRg7xv5huo/mKnBAcPL9Htt6b7QkxEUWyEvNcrczvpEMsz04GZATYi72IfuHnfIVZ0PsRHuO2jixo8ZQBL86rgxH92wDALh33oa0vuegBSMoV1xdmWLFcn8WwJQEr58GoJ/6bwaARzMfVgvEAVeBELF8nzfudLY1EzJxy1jzudtfUxAPM4Fx0kUmhFb7X/o+nBIlM8udc25rTCNquceKezAcQTjC0abAj9umDgIAfLP1AL7bU5vyfoznRFV9rPVO2q6QVNw5518AOJBgk2kAnucKXwNoyxjrZtcAWwp2BvmMAdV8nwdNNrVEbUmYpUJasU7Fd/3Rt3scGZeM2XCqbFiQJdn+Iob/AefEXf7cDbuVbop3zF1n63J34rjM3DJiVhrwe+GXMqdkN41VmgzX3y6TegFyyyjY4XPvAWC79LhCfS6n0KxJW4qYRCqk8pmKuCsn7R1z1+GphT9kvI+WQMTE527F1SK+6w+/3YMV2w46MzgVMyEwswbtQgitWbaMU24ZeR9T5iwAAG2xbLtmR3XNisVuVq/RoIm7B35P9FxIzy2TPHWVxF3BDnE3u/2afruMsRmMsaWMsaWVlZU27Dp7OGu5e7VUsWe/2oo/v/NtxvtoCYTMLHcLN0cmnVFOB5plHRA9cKrrnbPcNeExyZZxKqCa6Du3K9YjVigzc8uIczvf74XfF/1x08l1bw4nz5cnn7uCHeJeAaCX9LgngJ1mG3LOn+Ccl3POyzt16mTDrrOHyOO1tUJV+Nz9HttWqmlJRDgHY0CeN3qaWXLLSNP1/DRbF1hFFtdpI5QJ58EsiLuZ5e5UVotR7OSbiF3L39U1KedvMreMT7Lc07lxx1rusZ/htOHOOceH63a3+JuIHeI+F8AlatbMWADVnPNdNnxui8LOPHftM6VUSGORRy4QjnD4PEznZ00loApkp5BJ0K4wDwBQ1eCcWyaiN9yzkgppvKHK1q5dlrtYScxsAXlxAwn4PLobfTribrTUzW6ITovuu2t2YcYLy3DkrfMwd5WpHdsisJIK+R8AiwAMYIxVMMYuY4xdyRi7Ut1kHoAtADYB+DeAqx0brYuI4I897Qf05PtzN6DqYQx5XlmsrQdUAefdMrLl3r5IaeJWlSXLnXOOf322WXvNMZ+74TuXl3S047wLhiOaW+ZAXeyNUcxKA34vfN5M3TLJWwc77XPfcyhaGXvdf1Y4uq9M8CXbgHN+QZLXOYBrbBtRC0WcL/a0H4j2lgH0AdVcIhzh8HqYbipu5cKTN0m3B4lV5H0V5PkQ8HtMrU+7iAZUgZ3VjfjPN9u015zK6zfeNHZWNWh/2+GWWbR5P0IRjs4l+dhvJu5ytkyGlnswFEG/zsW4a9pQXPDvr10JqJoFgj//rhKXPv0Nvpp1Erq3LXB0/1ahClWLiAvEzvYDglzNcw9zRdwlbbfkV5bFaPozS7CmotqJ4QHQpyJ6GNC2IA9V9c3YXd2YcBHmdNEOjQPLf9RnAjklSkY3hRxTsMMt8+nGvSjwe3H2MT1woK4Z76/drXtdc8v4PfDLlnuabpmCPC/aFCizrFAkgndX78LaHdFzxA1X+EuLleyjldursr/zOJC4W0RcILb4gI0BVZ8H4QjXTTE5562+alVY7rJmhS3MfIxidM8857KHZEH1Moa2hX5U1Qcx9q/zcdqDC2zfn5wKuWJblelrTu1TcEiamTTYcI4dqGtGp5J89FQt1iv/3zLd63Et9zSupWA4gjxv9CbRHOa45qXlOOOfC7Vt3Ah0ivjZ/rrmFtNymMTdIpq4O5HnruaBy9b7c19txcA/vY89h1rvog7hCIeXGcU9+fsiEY6TBnbWHqdT7JIOjDG0KfDriphOf2iBrTdZ2ee+o6pe99rOKmd+6xhxb5Qt98yNlfrmMArzvOhQnC99bjjm74DP4HNPw3KvawrD7/VoNwkzY8t5t0z81/701lo8/sUWR/dvFRJ3i0Q0t4wzee6AfoX4t9UofMXBBqTD4i37s1K+n4iI6paRr4X5G5JXnYY5R4GUAun1OHeayhkqXo9iuct57ut2HsKWyjr79idVphqDjxc9tdi2/cjEWu7RgKodN64GVdyL86MhPHl20BiKumXyvB6cObw7gNQt9wN1zVi7oxpH92qj3SRMfe4uezg/Wb/X3QGokLhbJOqWsb+3TMDEco+S+v7W7qjGeU98jXvf25DB6DInFBZumegxPP75Fiz8fl/C94UjXJvNAMDBumbUNzuzmIn87Wo+d0dTIaN73G/S1VB8VwfrmnHH3HVaFkomGAOqcsDYDnGvbw6hMM+Hsg5F2nPy7EAYLfl+LxhjuPPMIQBSt9wXfF+JUITj9GHdNMvdrLuk/B1bcdH8v69/xPYD9Um3i4exitpBWyQlWsgwWj7iJLFj0WazgCqgF/dMHBEilc+JgGAqhLmSCtm1TUD3/P4680UWtPdFuK6qdc2Oapz9yFeOjFEWAo+HobTAp0t1s31/ktjsq43dj7Bmn1/0I579aiue/Wprxvs0xjl0bhkbAvn1zWEU5HnRu0MhHr/4WADmNxBhxOSpdQ+piru4GfZuXwhfgtRk+TdNlt1W2xTCH99ai18++bWlMeyoaohxu5z9L/25WdMYahHxMhJ3i9hpuctrqAJKQBWwr6BEWA5uV9BFIhw+L8PRPdviz9OGaM/LRUpGOOeIcH0nScDeFZJeWbINb63Yoe4v+ryHMXQqyY/ZPtF6nakiW9FmfVjq1UpP0avFGHRNa58GfZOFt8kWy11xywBAl9JAzD4agxFdpbIQ97U7D6UUfBStDYrzffCpn3X73HUx2+nEPcn1KvZ/IE5veCOXPbvE9KYss27nIZzn8KIvViBxt8A7q3fi+71Ke9JUrI3mUARls97FAx99p3teXkMViAZUzcQ9HX32qh9sV1xp/a5DmL8+9Q6NITWgCgAje7fTnk8k7uJ4C/KcOzVvfn0Nrn9lJQB9zrKXMVw45oiY7d9eudO2Yqpkv4kQ9c3q+fbj/sz9/Ubr9ZDtbpmouIsURaPlHvB5tQQCYXX/b9VOPK82MLNCTWMQhXle+Lz6Slcj8uE+NP/7uNtt21+PYXd8CMC68/NgnKZyxlN6lYPpu1YhcbfAtS9Fq9BS6QEjtn3a0OXRGFAVLgi5clBcCFYWJzAierPYVfF42oMLcNlzS1N+X4RzbSxylkSi5BezpfmcRP6GGAOK8n0oP6KdbpsnvtiChz+JLxKpkGw2JXq0bK5UxN2O3vJGz4Q8Y0iULdMYDOO9Nck7iTQ0h1DgV4KppQHlf13QNhTWdQZlkhIu/dF618+axhBK1M+Xzycj8nmfKHPljRUV2t9WLxWXcxRSgsQ9CcaMk0SWzhvLK0wLboypWdoj9fwsyY+1dpapJ/0vn1yMj1Psay7E0223TCjMNSvNJyk6S2i5K2P2Jrh47UTnczekpsrsNOkbnun+zKhrDoFzrvUptyMPPZ7l7vWwhOfzXe98i6teXI7lCdouc85RHwyjKF+5GZeaWu4RXQxFJpV+OjVNQZQElM/3JbAQ0skzt+p6ayk57FYgcU/C94bVYhJZOjP/uwo/fThaTBGRClZ0iPYDqroLayReN8LXl1eYPh8PcT9yu691RA2oAtbTGcUNyZvgBmAnsraIfHqzWYNdozG2GDbm8L/49Tac9ciXWnDdjiwhY62EEPc2Bf6ENw+RQWLWxlfQFIqAc6BAdcv4vR4U5/t0/Xkag+G44p5KOw/Zck9kIIjzv2NxPnweFvcGks7l4VT/HycgcU/C3hr9hZGKj1IUPMU7H8T5KU7Y6jj+vFQtcHHBuG25iwpVQC/WicYlLp5sFS7pA6rK/2aWu13Ixz6ke5uY43x9eYXmr+3RtgCNwUhG3SI557j59TW656obgsjzelAS8GWcaincRoWSeHcuycce6bpJJO6pFFEdagxplnsihFFz2tCuCEU4DlhYfMW6W8Z8Q7eE7S6hAAAgAElEQVSvNTNI3JNgtH5TEXdtMWTD88bHxaq4x+tGmKoFLvZr9wn34mLrwS9ADagKcZc7Q0rWWlV9s245PSFkiYKumWCcVsuPPYksdxuGE45wfLAu2nelMM8Lf4KbWI92Sjl/Jq4Zs4rqkFpHUBrwZ9wkTcwsCvOiBUxd2wSwW3Jj1TeHUZRnLu7JMk9kahqDmiGUCHG9iBRcK1XeVq+UeJfijqr0ig2dhMQ9CcYASiqWRtRyNwqK8r9coZrn88RduzN1yz3xjCFdbntzbUq586JCFdD7SOXMk6tfXI4rnl+KyhrlItfcMg5Z7nLjN865oYgpvs/dDp79aiteXRZ1sRXl+3QLkxjpqYp7JkHVeOdOwK8038pU3BvUsRVI4t21VC/udc1hFOabi/Ku6kbLM5OaxpAWsDUjFI5g76FGTdy7t1XEfcMuC2m0GVru63a6W1NiBol7Eow/ZrxsGbNAi+ijbTwfoi1/oxd2acAXd+3OHw/UpzR9Frm9TvgHU/EBy24Z3QIc0heydZ+S6if6iosxJxK9TJB/v2CY69sPaGvamvncMx/PDkMriQK/N2FgsFe7QgDApU9/k7ZrJl4vJL9asGWWa58KdcItI4t7mwD2HIqKdn1TSOe2kTlQ14zFPxywtC/Fco/vlrnrnW8x+i/zccMrqwAAw3q0Qf8uxbj/w42m28uzMasBVfK55xBGl0hDHCvK7I4uToQYy139X76sSwL+uG6ZLZV1+OWT1vuOiIpEJ1b2SaXlsWgcBugtdzkDiRly8oXHxqmAaqP0+zU0h2NSIYFoUZndGA+pKN+r3fyG9WgTs72w3L/ddQg1afrGjQt1CHZWN1q23BOdRwfV/jjtivK057q2CSAU4Vpv9/rmMArz9eL++MXH4t+XlCPP58GnG5P3YjnUGERjMIIO0n6MvLpUn3gQ8HsxZWg37DrUmHT2a1WzKVvGYYLhCGb+dyW2VNYm3zhDjCeFkh1g7seMfa+iVDE+d+GWkS72koAvboEEAKxKoU+0GIsTVkaqMQczn7t8gzBW00YDqpmO1BzZrdYQDOtb/qpjNcvEsONeY/yIgjyfts/7fzEc3dsEcN3J/XDb1EEoCfjQUeqyGM+oSIaxXcaYPu21v0sDfl1BUzwSNfiqVH3mnaSx9lBb/247oMzK6ptDKMrTu1N+MqQrTh3cBV1K87HXgk98l9oxM9FCGMbYhIcxtC3wg3MkPU6rV0pLDJzGI3l0ogWybuchvLF8BzZX1uHta8Y7ui+zYGZTKDZv16wDY9JsGelyD/i9WJ2kqm3R5v0Yd2SHZEN2LKAKRItsrBDmMM2Wkb8r4a4JGWYbTgVUZQFoCIZj2g8o/zuy65gbRKG0YLSHMXx1y8naa1dM7Iuvt+zXHtelmRJpPAeK8n3o3b4Qxfk+lBb40RSKJMxmARJXZYt+Lx2KoxZ1/y4lAIANu2tw7BHtdRWsRjoW52s3iETsrFZcWqmscuT1MLRTl048WN+sm10YsWqRtyJtb52WuxDcRhuq95Jh1pvCzIoy2y5eXwuzZxPlEgsu+Le15kZivxUHG2xfGaa2yXoALhyJRMVdUky5B7d4tjmkvyE5FVDViXtzWJ8tw8T/sft+ecl2vPB1atlCRowzAtktY3a4siD+7b0NabUKMM4oPYzhk9+fgHevm6AVHCWzahO1jt5X24TCPK8uW0a4k257cy0WfF+JplBE97pMp+J87KtJnqoolgbskYK4Mwa01RY9d27pxHi47cJpleIuTsaNe2qwYbezUWoz14ZZUDVZ61EZbkyXAfDXc4YlHYvcTyMUjsTk4Atka+2sR760pW2soDYVyz0SFUpdtox00xOCJ6b+Tue5N+os95A+WyaB0ALKQgx2UpDn074XM4tQFvcPv92DF1LowyIQPvdTB3cBoBybz+vRFiYB9F0izUgk7vtrm3RWO6D8pqI52P/UdQniWu4l+ZbSIXdWNcDnMW/sFg+v6pYBYJqsIF+eEW7/EnlOr/+bjNYp7pKVO9WBpdBkzIJJZumQppZ7kjmcbMiN6NU24bbnlfdCcziizRpun7sOo++ZbyrcxhvNkNs/SPjZqVCbQnZFJBJtPxDXclefFgIiu2VW3T5Z93myJVRZ05TWYiSy5f6zRxdhrio+Yp+6QdmM8VOL8qKWu5khkOfVC6JIiTzUGLTcf1y4u8xmJX3U/uuvLduR8DOaEwTR99U2o0NRrOC+89sJAKKrSxkDqoKOxfk4UN+cdPnKnVWN6FIaSOmm72EM7YTlHidZQeasR760/NlWcHtd5FYn7rVNIXwidSh02gdmarmbTI/NxD2ez9vEcE/K0B6lAKK+x/fURYjNfLFm++WcY8H3lSlPFY3bp+KWCUluGdkl8a/PNmtBNCE2okGauCH6PIplKevswk3KIh9rd1Rj1D0fp2VJG1158kXvddjnbvzBA/6ouJudP22L9Gl/Ylxn/nMhjv/7p5Z2Kb5PEdCU/c7DerbBpAGd8NG3u03fK0jUvG5/XbNpBkv/LiUY06e9NrM2BlQFnYrzwDnwn2+2JRzDjqqGpC6Zfp2LdY896spaQPzWHqmQKGvoqM7F2PDnKbrn1rrcGbLVifv89Xvw1sqotVUcpzjCLsx+ULOKQdlaFoIY3+cem+eejH5qkEpkDYh3NjZbm0X8d+l2XPzUN3hzRWIrzYgx9bE2BRdPhMfPV//9q0ousnj1updX4GBdszYTKVJ/V/necvFT32DG80u1xZD/J1ndVknU1ZOpV4NTwVxjrnzA79FSIM3O49KAH1/NOkl7fP9H32Htjmps3W991aCQ5Jb50xmD8cfTB+le71ScnzRInsi90BQKIxDH5dK7fSH2qQHXgjjbjOmrJAi8vGR7wjHsrGpAt7aBhNucorqeBB6mfIceBry1YkeM0ZOqoZPo3OnetiAmKJ1K+rITtDpxP7KT/u7csTh+BNwOzKzgZJa7mI7ZZbn/68JjNKul4mA9GprDmsVeH7RmuW9Tp/Fvr9yZkkAbL2wrgV95HPGKdIR7SQjpvtpmPPLpJm1sxXEqET+UWhWkI8JNqkvtiuP7xLzmtOVuHG6e14s/nzUUr145Dr07FJq+p8gg+uLGBlgTJ3Eu+L0eXDahT8znFeX7kmbiJLLcQ2Eet7f60ZKrMZ7l3r9LCS4a2zth+X44wrHnUGNMpkxng//97JE9dI+9HgaPh2FUWXus2VGNTXv1qdOmrRkS3MgSVafnZamLaSq0anGfOqyrLf2uEyEbrkIQzPzOsv9XnETJ8sytatPUYd1Uy8CDTXtrcdL9n2knmpnVZXbSbj+gXDyff1eJ37603NqOEZsGt2lvrWWLJxzhcQVYrIGprxKMzgxKLMzI0jGwxe8kgokyYqyjytrHvGYHxuG2KfQj4Pcm3F+8QCRgzacrfO7xWigX5XtR1xQy/U2tLAofDEfi3sAvGNULA7sqM852RfErS7u3LUBVfTBu4P/tlTsQDPMYcV9w84nRv/9wopaCKRC/529P6gcgNqhqZgQZFy2XSZStlCiV1C1anbjL07te7Qoz7o2RDOGWWX3HZFwyrgyAeXBGFlRhXRnXrhSkEybwehiO6lyMjXtqtF7fgHlaptl+5cDh8hSWbjOK+4bdNViy1doCC0oRk/lrYrEFWfwZojdOo4VpRjotCoSbqdRM3NWxjunbAV9LOecyy360ViqfjHnXHW96gzHiT1DNlawuAogKWDwBLsr3IcJjbxR3v/Mtvtyk5Nk/9MkmLN1qftzBcAT+OBW9Pq8H7153PN757QQM7lYad4xiVrozjvX+RzW2MsAg3nKbCHE+yTd8cW6ZrQ6ljD32SkyUcx+vgVv/LsWY/dPBpq+5mQ7Z6sQdAAZ2LUFZh0JdEYZTaKl5jGnBqKqG2Lu72XQurs9dc8ukJk59OhZjwff7dM9d9NRiS9NNmVS6TJoVsPywz1plcJhzXR/3LX+Zqv1tZrkDSOqWkUnHLSMs2USWe7zXASXDJl3k4YqOj5nwi8cXxfz2RoSAxcsyEe4So6vuScPqYY9+tjnu5yfqbOn1MAzt0SZhfEnkxVeYiHswrFzfvyjvidF94s9wxPm06Z7oOSaGJYKqRnE3M4L21sQX93g6c/dZw9C5RIkHvH3NeFx74lGYddpAAMCUOQuSZgI5RasU93nXHY9Pb5ykXYB3vfOtLWtNmiEX1RTleeH3spjIO+ccT3/5Q9z3GjGuoZqI88p7aX+fY/ApCt4wLOYhbipm/UrUAVhm9tzYjJR9FhcTNlrusqX9yYa9uOfdb3UzD46oTz+ej1bGqCmcc1zwxNd4dWn84FxCy136QfJ9nrjpqVaWnkuGXXn8yUr3o5a7+aUuZkj1SYKqeXGs82A4knB2YQXhbtlxsCHmJrPjYAMiHBjdJ3Fltl89Pvl7FX+brQ4FmBtBxuZuMvF87vJPObxXW9z4kwHa97pxT40rBVRAKxV3j4fpijBeWrwNN7++2pF9yXnXjDG0LcyL8d2tqqjGvDX6dLKG5nBcn7tVw7l3+0L87dyjtccnDuyMSQM6xWxntGBDEY48n0dLn4zZv7XdAwA+21ipe1yc79NKzpMhNw4z498LfsCWffqbcl1TSJf/nRhlm8ZgGB9/uwcb99Rg0Zb9uOm1+OeC8B+XmnQXlHfp8TC8dc14/POCkdpzF47pDQC46sXlaU235ZlaKtr+wmWj47529UvLsS1B9ozmc4+zw+L82PV7zYgn7qEwhy9Dce9cEoDPw3D/hxsx9PYPtE6hALBVNdrK4gScBWZrqorZQkm+D4zFVuKazay3JagfiGe5mxkKcszIziLCVGiV4i7oKU1tnXJtCYEWPst2hX4crDNO76I7F2P63+qdppZ7Q3MY//eB0oI0keXeuSQf//3NuJjnzQJsxus2HFGCXDNPHYDj+3WM2T7d5fcYU3qIWF1gIRLRu2WsUNsUMnXJ3DxloOn2Dc1hXPTkYlz+/FI8uUCZPR3ZqSju54fCHB4Wm3r482N7mroOfjq8Oy4YrcyeREEMkF6Bipk/2ArH9+uED66fqM0kHpJuOFX1Qdz5v3V4d/Uu/NcknTCaLWO+P9EWIFkrZ7OMGM45msORjDNFvB6Grm0C2ox4qzQLF2Lbu31icU80e/B4GEoDflQ1BHGoMai5Gs0s90Q3SjNxf+7Xo2MCuYA+ZpRKhpmdtGpxH94zOm2uONiAV5dut909o1nuqoK2LcyL6d4on/hnjVBcJ394bbWuklYUc8jNoBL53KcO66atJCNj1mu80SA0YgWkTiX5eOGyMbh03BG619Ntb+phDB2K8rC/zpq4hxIEVM149qutWLOj2jTn+xflPWOe21fbhEGz38dSdTFxUT6e6PiCEcWNYLxJ/t/Ph8d9z1/PORpb7z1d890C+kD28m0HLblq5F871XjBgK4leP2q4/DNbSejl8Ff7/UwXPPScvzBZPYaiiTxueeb+9yNmFnucpplpshtloX75KtN+/DQ/O/h9zJdh0yZM47uBiDxgtkAUFrgw8tLtuPoOz7EZc8tAWC+fmtiy13Z/oPrJ2rPndA/diYN6I0H0d7hua+24u2VqdWZZEKrFnePh+GZ6aMAKBVsN722Gj/950JdOtPVLy7D5c8tTXsfYWk1IQBoWxC/7zqgz7uXp4FT5ihtEuRrOt713b9LMa496SjT18x6je+oatAVW4UjXHfBTR7SVbe9VctdnMw3Tu4PABjesw06WGz0BCjfXSoZLeEIx7qdh1Bs4jJpV5iHrfeenvD9IriYKCgWDHFTcbdCW8lyr5esuHP+9RWuejF5eqnspkvH5e71MHQuCcQUBCVyYQnXQ3yfu/JZyaxLs1moiF9k6pYBoqm6QLRH/C+fXIx9tc3oXBKIex498IsR+Oa2k5OeZ9sPNGgWu0hKMFru/bsU44d9dXEDoCJbJmBhpS5Z3EUG2O1z1+F3L6/EC4u2Jn2/HbRqcQcUP3T5Ee0AAGeN6I7aphAe+zwa2Z+3Zjc+ltoVpEo4om9X27YwdoEDudCnU0nU2jZryJTIYhMX6b8uPCaupXLDqf1jhOnd1bvwz082aY/ltUuB2JPRqlNGWHNtCvx4/apxeOZXoy23aAX0vWVSoVjqQ/Lr8Uptgbh4N/9lqta3RGAMfNY2heL6OUORCHxeFrdLYSLaSr7V+jT8qLJmZBJQLfTrx55Q3JP43EXg+rf/WZEwn92snkRUZcdz+aTC9af20/4+YDCe4lW3AsqMonNJ4spVADh/VC/d44bmcIyIH3tEezQEw9gUZ50I4ZYpsJDTLrsWjTfOP729Lun77aDVizsAjOytXNy3TB2EM47ujpcWb0N1g+KLzJQI55CNnsI8X4x/Uj5J5GKNhYa0RSA2r1tG3EQS+am7lAbw0hVjY56fJ7kFwmG9qBoLLKwGA+W0xGOPaI82BX6UdSjEgbpmTHvky6R+2pBJQPXZX41Kut+jpEK12T8drLPYRWrd09PLAQCXjjsCfzojWlJ/lNpf5LLnlmDT3ti1M4NhxXK3Yn0ZkW+qZmK3YttBbE6wgIycepdK6wkjgTz92BMZDMny3NtIriYx0zSz0r/esj+mC6moXLXDLXP1pKM019dBQyGRHanO9/7saN3jQbPfjzlOYSTGWxhHjCPf78XT08sx89T+cfcnu7FqGoOupEPmhLjf9JOB+OKmE9GlNIAZE/uitimE4Xd+iGe+3Kptk2jBgUQYMz4K8rwxKVFyMcQRHYow4SgliGlcNPfTjXtx0VPRfhPGC9xsYQszzFwKfTpGg4hBqWEXECvu4QhPKvCNwTBOvO8zAPq0xP5qxeGq7VVYuyN+u2VjrEIwaUDnpG1bj1EvskScNLALtt57Ou6cNhTd2kR90KcNVVxQX285gIfmb4p5XygcgV/NtkoVOUg2878rY0Tn7H99hZPv/zzud2vX9W2cdcxN0GNHa8QWx7ouDfijldfqzdysHcHemiac+U9918SQjT53QfvCPByoN4q7M8JoLGI6snMxSgM+3Pz6Gjy5YAs459iw+xD+8dF32FnVoLPcTxrYBded3M/sYwEA3dsE8Dv19dqmEDbusbBIt83khLjn+Txab46hPdpg6rCuMdtYzfAwEo7o/cYFfi+awxHdnVhuGub3MFx/ivmP/pihEMR4uWntcZNMc4Vwl3UoxJtXHwdAf0Eae7oYxT3C9T1azJCDxvIUU5STA7F5wzLGLCPda0mKrBIVq5ghi+7YvtF8aDNBC4Yjmo94yW2npLSf4b3a4qpJRwIANlfW6WZLMn1umWf6fLpZSkYSuQWMrpVkee6A4o4AJHFX///rOcOw9s6faN0Wdxty6oXBFO/GkQ7tivKwr6ZJF7AWN+xMMX6O8Tz0e5VZIQDc/e563PDKSkyZswAPzv8es99eh8ZgBB5mzQ3FGMMNp/ZHvs+DypomnP7QQt3r2ahczQlxN/KvC4+Nee6L7ypNtoxS3xzCB+tiW59GDAFVYTXLATW5sZLP64nrzzVe3EbjUYh6Msu9V7tCjOjVFn85ZxhG9m6HUwZ11hUWGX3uZmLw/5KsKiT3rJGDQ11Lo/7NO+au026aB+ua8Zm00HE4juUOJO5VsuJPp+oscSsUSTOZ9kV5+OzGSSgN+LByWxV+88JS3TQ/GOGaGKWy8INALiRL5A4xX2fXHgvU62HoZpJJBcSWyAsjJJFfXvy+IvB31/++1Z4vzvfFrXgWz8drHJYOZR2KsPiHA/hQbUP8+1P7xy3tT5U554/QCbzxPGxXmKdLWJC7z/q9DA3qcoSpzPpKAj7Tdh+pLDSfLpZ+FcbYFMbYRsbYJsbYLJPXpzPGKhljK9V/l9s/1NQQGR6CWW+swTUvLdcJkMwDH36H37ywDIulVEUg1i0jrGC5L7h88vu8TMtAMGLsyRLjlhFdCZP8Knk+D966ZjyOO1Jx/7QvysP6XYdw73sbsOzHA3h39S5dEMfoXz55YOekhUhyapws7owxfH/PaQCULJ2731GE4LLnlmD6M0s0P7xW2WtyISSy3OV0Q6vI2RodivNQ1rEIpwzugi376vDBuj34SJqlhMKRjMRIDu4xBnzzg3nPlRqTgKudbtdFt5yMSwwprkBsv/pQEp87EJ2Zidmf8DmLWFa8GZoQRzstd1G2L9o5D+lRapvbJ9/n1bkv5fPwnd9OQPe2BbjxJwMwtm/7mA6T763djacW/mApmCrTsTgfa3bE9gBKZ6GZVEn6rTHGvAAeAXAagMEALmCMmd1KX+Gcj1D/PWnzOFPm2pP64ViD7/bd1bsw/ZklptuLE3v9Lr0fOWJI59Msd+kikn8ov8cTE90XRTDJ0FblSfF3Fyf/Y59v1nqfyOmAASk3/rMbJ1nKeKmLI+7y/oDosnvrdyk+xcGzP8DybQcTLpeXqPdNJoFGIFpodJS0cIP8+ykVlenvQ44//O7llfjF4+a9ZsxunvEayaXLXdOGxtyoZr+9Tjdr0G6yCY5Z/L7CIAhGOM4f1Qs92ymuznji3mxjQFXQqSQf7Qr9+GqzYmSVdYhfkJYOcjqqqI8AoLljhnRvg5dnjMNlE/qYvj/V7o8XjY29AQMtRNwBjAawiXO+hXPeDOBlANOcHZY9CGvlhctG45xjonfi1RWx0yThCjAuhGC03M3F3WC5G9wysh84EcPVlL5UU8vMOkPKyOJW1rEIHUvycKCuOeHKMrLlblZeLThY34xwhOtcTl98V6mt3Wkm7p3UNM/fTOwLQEll3PKXqdikzggyQQjNL0f31p574esfNfdRcziS0P+cjERpeTIHTAq9nLieX/mNPnPq/XW7dXUYWtAzwTELcReuuJrGIEqkOIscwP/3F1tiPttOtwwAdG1TgPrmMLq3CegsbTsIhqy5Q4Z0L8VFY3vrYkyA9d9fcMHo3rh60pH4zcS+OpdeNtZXtfKr9AAg1zVXqM8Z+RljbDVj7DXGmDVT1WFEOlIownW5sGc+HJvGJ+6kWw0VruEITDNPZN+mfBf2eVjM1M3qalFzzhuB164chw5xctzjcdOUAbjpJwMwKEFbVZmOxfkIR3hMpW19czQ/XPz/+U2TTK2Vnx2jVIwu+/EgHvhoY0xVaCLL/eUZY/F/5x6NCWprBBG0tqMYRtC2MA8r/nQqAMXNcMMrKwEkXlzCCmZFZGaYWe52BVRlRvZuh+OO1BsP8sIXcuO7eAi3TG1TUO3CGEFxfvSG/vpVx2lB1Xvmrdeed8ItAwBdS5Xzf/xRHdOayb11zXjMOW+E6WtW4x6MMdx91jC8f/1EXXyjvcmSgonwehj+MGUgbpk6SHd9xusYaydWzlSzb9c4sv8BKOOcHw3gYwDPmX4QYzMYY0sZY0srKxMHOO3gz9OG4rShXTGubwdcc+KRulLhF7/eppu+CuvXmFVjlucubw8YF3xmMUHEOosLihTl+1CexkIR3doU4JoTj8K1J5pXtRoRgURjd8cxf5mPIbd/gGkPL9Qs93h91e//xXD8+ayhAIBHPt2ss0S+31Or3RzMRKVX+0L8vLyXdtNI1qI4XeT1QitVN5UoYkoXq2Kz32TRB6eO0/gd3/Puek14oxWq8cddqP4OX3y3T3PNyJZ7/y4l+LWJm0Lsw063DACt7cYEk75IVhjRqy3OitNBdfLg1DNvZN98pxQNLxk59tVS3DIVAGRLvCcAXWIt53w/51yo4r8BxKarKNs9wTkv55yXd+pk3pPBTso6FuHRi45FwO9FScCP+38R7R9yz7z1uvxgYYmbNQUzd8tELX+xOvyq2ZNNx2G0rJyic2n0xLvf0CtlbN/2uE5taSCqX8Vi2wDwu5dXaBf2qopqzFar6BLNOi4ee4RW+CHz7ppdmPlfZY3URJk/wgq2wxf94Q0T8dENE+O+LiyuoA1dDGeo7qRE/P39DTEXcCI3WCYYxX3Rlv2Yv15JHAhFImAs8cIm4rWFm/Zhrtr7pMTQvO3cY3uifxfFehdxKeGOtKNCVaZnu0J4GDDOgetmQr+OeOJiU3mKizzj6pDBsp750gy4pYj7EgD9GGN9GGN5AM4HMFfegDHWTXp4JoD1aIF0LM7H87+Otk9dv6tGs7qFJR7T0N8QUDVzy4SSTE87FuenfEKlQxfJ9fSzY/WNtl6eMQ4zJw8AoASPCvO8eGeVkqfNOcfbK80LYZK5IeKd7MvUYFUiUbHTcu/fpURbRNwM4aILqkVMmXDr1EExzxlbMR+sD+pa1wLJ8/vTxewGKmZOoRRbQOw+pNhoJYb+Pn6vB9PUpninPbgAc1fttLVCVebicUfg1SuPs9RWIB3kVOXfnhRdWCMe8vlpvOmlgpw622zR958JSX8VznkIwLUAPoAi2v/lnK9jjN3FGDtT3ew6xtg6xtgqANcBmO7UgDNFtqIf+3wz/q623xViXdsU0lWzRuJY7g0m2TLySS6vWg9YW1koU2TLPRHF+T6ce2xPvL68Ak8u2JIwcyaZGyKZDzKRsIjvyynRA6Ktgj//rhJbKmsRCvOY32nBH07MaB9zzhuBZ6aPwh9PV0Rf9B43fq9OuWXMbqB71FYBYUPNQzyEX1m0GCg1OV/l1amu+88KvLpMCcVlEqA2ozTgj8l0sxMRFC0N+PD7yQNw5QlHJtxePj8zOVbZDdxSLHdwzudxzvtzzo/knN+jPjebcz5X/fsWzvkQzvlwzvmJnPMNTg46E4xTclG4JGe/yMvoGS8OESyd8/H32o9lNj01LuZrNaiaCQG/F6cN7YrHLko+S7h16iCcOrgL7n53PUbfMz/tfZq1IJZJJCxC+J0MLl016UhMG9EdnAMn3f85ggafe/e2BeiVpFe4GbILqGubABhj2ixHxDSMQVUnAqpAtMPkT4Z0weo7JqMk4MNe1QIPhXnCTBnB+2obW7GOqdFyB2KXHhRWfp7PXreM04hrOC/JuSvQi7s9x9pixD3X+PymSdrfHVTLU3azyKlkEc510x6P6ugAAAtvSURBVClx1999qBEV6pJcYgX4RFZuNsQdAB696FhMsVCuHfB7Mf24soz3lyz7wEoXzAxT25Miz8SMlnu69OtSomXdiN9WuH56t1fS9yoMS7Y5NUMR1uQZR3dHacCPLqUB7DkkLPdI0nYWQPQYdlYp7zNzP8RbV9Zut4zTiGvY6iIj8u82NoM4gPzrZyMVMjuK08I4okMRrjvpKDz0ySYs31aFFxf/iIbmMPK8HjSHI7pydaPlnu/zoDjfh9qmkNbSNxRJXhiTDbdMqhhnF0ZW32EeIJZJJlhmbY8F3doE8JuJfXGuIT5gN9sPRmsXxI3YFtSPEcJ49sieqDjYgCsm9sXryyvwt/c3oE/HQvRuX4RgOOKYuAu3jJgZdCnNxzc/HEBTKKy0W7BwvF4PQ3G+D9sO1MPDzGMpRnEX1bB2prBmg+jC2daCo5MGdMIH6/Zg1ezJui6aqSKvENZi2g/kIjMnD8DkwV0AALe9uRYNwTC6t1X8jvIC2GGu92kyxvDEJYrb41CDErRqDiVfJLgkP/2Twini9ScBlOXDzNYZNXLJuDKUBHxYdMtJpq/vro6/gDNjDLdMHZQwEGoHt/90iPb3wfpm+C3mqidDnBXixp3n8+D3kwfovrfrX1mJqQ8twLRHvtSCzHYz89T+OKZ3W5w4sDMApWPm/rpmvLDoRzQFI5Z7xwtrfXSf9qZuGWO9g1iRy+5sGafp3b4QM0/tj8ctJjk8eP5IfHrjpIyEHVAatD14vpJ/H0yzS20qHLbiDgDnHBO1GDftrdWqVKvqm7Fk6wEs+L5SDajq3ycu3qjlbi7uolgHiO3v8vHM+Gl72cJ4sYoWpcN7tom7fJiRQd1KseaOn6BbmwK8euU4PHlJue71dPL27WZUWXstsNoYzDxbRiCWeUzkcpPb1RqbetlFn45FeOPq8dp5edmEPuhSmo+1O6qxqqLKdI1PM4Qf+MQBnU1fP6JDoVbMBEBb8cyKT78lwRjDdSf3sxxrCfi9tlXKit+CfO4OM2VoV9wudZwT5+isN9bg548twsVPfWOabaCJu5o2qSzdFisYY/t2wM/LlRIBxphu7cWjOjtrraaD6K+RrhN8VFl7nDK4i1ay/eQl5ZZvEk4juxnsWrD435eU45UZY00reP88bYjJO7LHgK6leGvlTmzaW4uJ/az9BqKobXB380rngN+Lj2aeoD2OcCV4HM8XT8QijMCW0n4gp5FdE9/tqY3xT4YNAVVAWWwXAA41hvDVpn14Zen2pF0WAWWR45bGN7eerP3dLsNpp0D4fjum0VLXKeR8/USrJaVCm0I/xsTpG3TxuLK477v3nGG27D8R4jzO83nw0+HdU3rvgCSW/nnl0ZrGkwZ0Tmmd3MMdEYRvKe0HcpqTBnbB9af0Q1GeF7OmDNR1jQPUPHfDySum4Ycagnh5iZLr61QOs9N0Lg3gk9+fgH9eMFIz2DO9VEXgMNX2qE4iuybM2vE6wRiTRUe6lObjfKmpmVNcc+JRmDKkK1bfPlkr57dKsj73fzv3aIztqxzbSYPMXTiEOX41bZTcMlkgz+fB9af0x7q7puBnx/Y0bYBlFHdlQQ4vnlu0Na21OFsafTsV46fDu2sxh8lDumT0eVOHKQXLHTMo1babQd1Ksf6uKbh16kBLdQB28OSl5bjzTL17pr7JGb+7kWOPaIfHLj42pRa1H8+ciBcvH2Opf05pwI88r0dbUpKwhnDLZEPcW15+XgujKRhBaSD2ZK9vDqO+OYzXl+9wYVTO0L1tAZb/6dSM3TM3nNIfvxrfJ+UOek5TkOfFjImJqxHtpCTgx6XHlYExaL16zNYnbSkc1bnEcizovFG9MLZvh7iN5Qhzoj5352f69MskobohmHBam2ru8qrbk+eOu4kdguzxsBYn7G4i9/dvpd67GE4elNns7nAlL4uWe+v3KTjMjqoGmMWL7khzXcc2BX7KLjjMiLfsInH4IbLqKM/dBcya/Julzk0f3wdThihl/m0L/Vhy2ymOj41oncRbMJ04/PB6GBgjy90VzhrZAxv+PAWnHx3tYhyva2IftZy4V7vCpBkGxOELWe6EgDEGv8eTFZ87ibsJAb8XVxwfXZChssZc3HupCwjXZSm1jmidyHUSdq83SrQ+HrpgJM6Os1KUndB8MQ5yjna8isae7dR2BXFWhycIIJoh0bE4D3OvneDyaAi3sdK11Q5I3ONgJUdb9KYwLjRNEDJDupfij6cPwtkje6S8+DlBpAvNEePQoTgf6++aknAb0UWytTVOIrILYwyXH9+XhJ3IKmS5J6Agz4uXrhijVW4ayfd58aczBpuWmRMEQbgJiXsSjjsycXn1ZRP6ZGkkBEEQ1iF/AkEQRA5C4k4QBJGDkLgTBEHkICTuBEEQOQiJO0EQRA5C4k4QBJGDkLgTBEHkICTuBEEQOQjjxkVDs7VjxioB/Jjm2zsC2GfjcFoDdMyHB3TMhweZHPMRnPNOyTZyTdwzgTG2lHNe7vY4sgkd8+EBHfPhQTaOmdwyBEEQOQiJO0EQRA7SWsX9CbcH4AJ0zIcHdMyHB44fc6v0uRMEQRCJaa2WO0EQBJGAVifujLEpjLGNjLFNjLFZbo/HLhhjTzPG9jLG1krPtWeMfcQY+179v536PGOMPaR+B6sZY8e4N/L0YYz1Yox9yhhbzxhbxxj7nfp8zh43YyzAGPuGMbZKPeY71ef7MMYWq8f8CmMsT30+X328SX29zM3xpwtjzMsYW8EYe0d9nNPHCwCMsa2MsTWMsZWMsaXqc1k7t1uVuDPGvAAeAXAagMEALmCMDXZ3VLbxLADjun6zAMznnPcDMF99DCjH30/9NwPAo1kao92EAPyecz4IwFgA16i/Zy4fdxOAkzjnwwGMADCFMTYWwN8A/EM95oMALlO3vwzAQc75UQD+oW7XGvkdgPXS41w/XsGJnPMRUtpj9s5tznmr+QdgHIAPpMe3ALjF7XHZeHxlANZKjzcC6Kb+3Q3ARvXvxwFcYLZda/4H4G0Apx4uxw2gEMByAGOgFLT41Oe18xzABwDGqX/71O2Y22NP8Th7qkJ2EoB3ALBcPl7puLcC6Gh4Lmvndquy3AH0ALBdelyhPperdOGc7wIA9f/O6vM59z2o0++RABYjx49bdVGsBLAXwEcANgOo4pyH1E3k49KOWX29GkCH7I44Y+YA+AOAiPq4A3L7eAUcwIeMsWWMsRnqc1k7t1vbGqrM5LnDMd0np74HxlgxgNcBXM85P8SY2eEpm5o81+qOm3MeBjCCMdYWwJsABpltpv7fqo+ZMXYGgL2c82WMsUniaZNNc+J4DYznnO9kjHUG8BFjbEOCbW0/7tZmuVcA6CU97glgp0tjyQZ7GGPdAED9f6/6fM58D4wxPxRhf5Fz/ob6dM4fNwBwzqsAfAYl3tCWMSaMLfm4tGNWX28D4EB2R5oR4wGcyRjbCuBlKK6ZOcjd49XgnO9U/98L5SY+Glk8t1ubuC8B0E+NtOcBOB/AXJfH5CRzAVyq/n0pFJ+0eP4SNcI+FkC1mOq1Jphioj8FYD3n/AHppZw9bsZYJ9ViB2OsAMApUAKNnwI4V93MeMziuzgXwCdcdcq2Bjjnt3DOe3LOy6Bcr59wzi9Ejh6vgDFWxBgrEX8DmAxgLbJ5brsddEgjSDEVwHdQ/JS3uT0eG4/rPwB2AQhCuYtfBsXXOB/A9+r/7dVtGZSsoc0A1gAod3v8aR7zBChTz9UAVqr/pubycQM4GsAK9ZjXApitPt8XwDcANgF4FUC++nxAfbxJfb2v28eQwbFPAvDO4XC86vGtUv+tE1qVzXObKlQJgiBykNbmliEIgiAsQOJOEASRg5C4EwRB5CAk7gRBEDkIiTtBEEQOQuJOEASRg5C4EwRB5CAk7gRBEDnI/wdreizYakuywAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff4e1e6a860>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d_losses = mnist_dcgan.disc_losses\n",
    "g_losses = mnist_dcgan.gen_losses\n",
    "\n",
    "plt.plot(range(steps), d_losses, label=\"Discriminator Loss\")\n",
    "plt.plot(range(steps), g_losses, label=\"Generator Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCCCCCCCCCCCCCCCCCCCCCC\n"
     ]
    }
   ],
   "source": [
    "new_mols = []\n",
    "\n",
    "for line in GENERATED:\n",
    "    for sample in line:\n",
    "        new = sample[:11, :23].reshape((11, 23))\n",
    "        new_mols.append(new)\n",
    "        \n",
    "found = set()\n",
    "\n",
    "for mol in new_mols:\n",
    "    s = convert_to_string(mol)\n",
    "    if s not in found:\n",
    "        found.add(s)\n",
    "        print(s)\n",
    "        \n",
    "f = open(\"epochs_\" + str(steps) + \"_rms_8.txt\", \"+w\")\n",
    "for item in found:\n",
    "    f.write(item)\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
